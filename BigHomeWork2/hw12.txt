
# import gensim
# from gensim import corpora
# from nltk.corpus import stopwords
# from nltk.stem.wordnet import WordNetLemmatizer
# import nltk
# import string

# nltk.download('stopwords')
# nltk.download('wordnet')

# # Ejemplo de documentos
# doc1 = "Ayer fui al parque y me diverti con los arboles disparando a los pajaros."
# doc2 = "La economia del pais quedo destrozada"
# doc3 = "La guerra en Italia fue inmensa y la vegetacion quedo quemada"

# # Compilando documentos
# doc_complete = [doc1, doc2, doc3]

# # Limpieza y preprocesamiento
# stop = set(stopwords.words('english'))
# exclude = set(string.punctuation) 
# lemma = WordNetLemmatizer()
# def clean(doc):
#     stop_free = " ".join([i for i in doc.lower().split() if i not in stop])
#     punc_free = ''.join(ch for ch in stop_free if ch not in exclude)
#     normalized = " ".join(lemma.lemmatize(word) for word in punc_free.split())
#     return normalized

# doc_clean = [clean(doc).split() for doc in doc_complete]

# # Preparando el Diccionario y el Corpus
# dictionary = corpora.Dictionary(doc_clean)
# corpus = [dictionary.doc2bow(doc) for doc in doc_clean]

# # Creando el modelo LDA
# ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=5, id2word = dictionary, passes=50)

# # Imprimir los temas
# print(ldamodel.print_topics(num_topics=5, num_words=3))
