{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYI3fzugjeLv"
      },
      "source": [
        "#### HW 1: Download the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1pcuK5bKVFR"
      },
      "source": [
        "https://www.kaggle.com/kavita5/twitter-dataset-avengersendgame/download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5pFcpsoSe99o",
        "outputId": "e525a290-1a90-4135-92ad-f511acf43e4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-12-10 02:08:07--  https://docs.google.com/uc?export=download&id=1Hzdo6EkFw6cQ4Kw19lzL7mr_ez_3fswN\n",
            "Resolving docs.google.com (docs.google.com)... 2a00:1450:4003:80c::200e, 142.250.184.174\n",
            "Connecting to docs.google.com (docs.google.com)|2a00:1450:4003:80c::200e|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-0c-c0-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/avs889t9ni85nrqusqg8lrpdpuslvto2/1702170450000/17600825180070349509/*/1Hzdo6EkFw6cQ4Kw19lzL7mr_ez_3fswN?e=download&uuid=727b83b7-d239-4e84-b753-248d2c5849a7 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2023-12-10 02:08:18--  https://doc-0c-c0-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/avs889t9ni85nrqusqg8lrpdpuslvto2/1702170450000/17600825180070349509/*/1Hzdo6EkFw6cQ4Kw19lzL7mr_ez_3fswN?e=download&uuid=727b83b7-d239-4e84-b753-248d2c5849a7\n",
            "Resolving doc-0c-c0-docs.googleusercontent.com (doc-0c-c0-docs.googleusercontent.com)... 2a00:1450:4003:80f::2001, 142.250.200.129\n",
            "Connecting to doc-0c-c0-docs.googleusercontent.com (doc-0c-c0-docs.googleusercontent.com)|2a00:1450:4003:80f::2001|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4763398 (4,5M) [text/csv]\n",
            "Saving to: ‘tweets.csv’\n",
            "\n",
            "tweets.csv          100%[===================>]   4,54M  2,61MB/s    in 1,7s    \n",
            "\n",
            "2023-12-10 02:08:20 (2,61 MB/s) - ‘tweets.csv’ saved [4763398/4763398]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# I stored the tweets.csv file on my Google Drive and downloaded it from there.\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1Hzdo6EkFw6cQ4Kw19lzL7mr_ez_3fswN' -O \"tweets.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>text</th>\n",
              "      <th>favorited</th>\n",
              "      <th>favoriteCount</th>\n",
              "      <th>replyToSN</th>\n",
              "      <th>created</th>\n",
              "      <th>truncated</th>\n",
              "      <th>replyToSID</th>\n",
              "      <th>id</th>\n",
              "      <th>replyToUID</th>\n",
              "      <th>statusSource</th>\n",
              "      <th>screenName</th>\n",
              "      <th>retweetCount</th>\n",
              "      <th>isRetweet</th>\n",
              "      <th>retweeted</th>\n",
              "      <th>longitude</th>\n",
              "      <th>latitude</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>RT @mrvelstan: literally nobody:\\r\\nme:\\r\\n\\r\\...</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2019-04-23 10:43:30</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1120639328034676737</td>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
              "      <td>DavidAc96</td>\n",
              "      <td>637</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>RT @agntecarter: im emotional, sorry!!\\r\\n\\r\\...</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2019-04-23 10:43:30</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1120639325199196160</td>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
              "      <td>NRmalaa</td>\n",
              "      <td>302</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>saving these bingo cards for tomorrow \\r\\n©\\r\\...</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2019-04-23 10:43:30</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1120639324683292674</td>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
              "      <td>jijitsuu</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>RT @HelloBoon: Man these #AvengersEndgame ads ...</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2019-04-23 10:43:29</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1120639323328540672</td>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
              "      <td>SahapunB</td>\n",
              "      <td>23781</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>RT @Marvel: We salute you, @ChrisEvans! #Capta...</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2019-04-23 10:43:29</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1120639321571074048</td>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
              "      <td>stella22_97</td>\n",
              "      <td>13067</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14995</th>\n",
              "      <td>14996</td>\n",
              "      <td>RT @natsdany: First time                  Last...</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2019-04-23 09:22:03</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1120618828918951937</td>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
              "      <td>tommysboi</td>\n",
              "      <td>33</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14996</th>\n",
              "      <td>14997</td>\n",
              "      <td>RT @MTVNEWS: The #AvengersEndgame cast has see...</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2019-04-23 09:22:03</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1120618828038311936</td>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
              "      <td>kimberleywithae</td>\n",
              "      <td>2307</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14997</th>\n",
              "      <td>14998</td>\n",
              "      <td>@SPICinemas kindly announce the approximate ti...</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>SPICinemas</td>\n",
              "      <td>2019-04-23 09:22:02</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1120618823667920896</td>\n",
              "      <td>919079586.0</td>\n",
              "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
              "      <td>Gnanavel07</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14998</th>\n",
              "      <td>14999</td>\n",
              "      <td>RT @Marvel: We salute you, @ChrisEvans! #Capta...</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2019-04-23 09:22:02</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1120618823600803840</td>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
              "      <td>_moonljght</td>\n",
              "      <td>13167</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14999</th>\n",
              "      <td>15000</td>\n",
              "      <td>RT @Avengers: Welcome to the party, @RobertDow...</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2019-04-23 09:22:01</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1120618822300569601</td>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
              "      <td>CaterinaCabrel1</td>\n",
              "      <td>10736</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>15000 rows × 17 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Unnamed: 0                                               text  \\\n",
              "0               1  RT @mrvelstan: literally nobody:\\r\\nme:\\r\\n\\r\\...   \n",
              "1               2  RT @agntecarter: im emotional, sorry!!\\r\\n\\r\\...   \n",
              "2               3  saving these bingo cards for tomorrow \\r\\n©\\r\\...   \n",
              "3               4  RT @HelloBoon: Man these #AvengersEndgame ads ...   \n",
              "4               5  RT @Marvel: We salute you, @ChrisEvans! #Capta...   \n",
              "...           ...                                                ...   \n",
              "14995       14996  RT @natsdany: First time                  Last...   \n",
              "14996       14997  RT @MTVNEWS: The #AvengersEndgame cast has see...   \n",
              "14997       14998  @SPICinemas kindly announce the approximate ti...   \n",
              "14998       14999  RT @Marvel: We salute you, @ChrisEvans! #Capta...   \n",
              "14999       15000  RT @Avengers: Welcome to the party, @RobertDow...   \n",
              "\n",
              "       favorited  favoriteCount   replyToSN              created  truncated  \\\n",
              "0          False              0         NaN  2019-04-23 10:43:30      False   \n",
              "1          False              0         NaN  2019-04-23 10:43:30      False   \n",
              "2          False              0         NaN  2019-04-23 10:43:30      False   \n",
              "3          False              0         NaN  2019-04-23 10:43:29      False   \n",
              "4          False              0         NaN  2019-04-23 10:43:29      False   \n",
              "...          ...            ...         ...                  ...        ...   \n",
              "14995      False              0         NaN  2019-04-23 09:22:03      False   \n",
              "14996      False              0         NaN  2019-04-23 09:22:03      False   \n",
              "14997      False              0  SPICinemas  2019-04-23 09:22:02      False   \n",
              "14998      False              0         NaN  2019-04-23 09:22:02      False   \n",
              "14999      False              0         NaN  2019-04-23 09:22:01      False   \n",
              "\n",
              "       replyToSID                   id   replyToUID  \\\n",
              "0             NaN  1120639328034676737          NaN   \n",
              "1             NaN  1120639325199196160          NaN   \n",
              "2             NaN  1120639324683292674          NaN   \n",
              "3             NaN  1120639323328540672          NaN   \n",
              "4             NaN  1120639321571074048          NaN   \n",
              "...           ...                  ...          ...   \n",
              "14995         NaN  1120618828918951937          NaN   \n",
              "14996         NaN  1120618828038311936          NaN   \n",
              "14997         NaN  1120618823667920896  919079586.0   \n",
              "14998         NaN  1120618823600803840          NaN   \n",
              "14999         NaN  1120618822300569601          NaN   \n",
              "\n",
              "                                            statusSource       screenName  \\\n",
              "0      <a href=\"http://twitter.com/download/android\" ...        DavidAc96   \n",
              "1      <a href=\"http://twitter.com/download/iphone\" r...          NRmalaa   \n",
              "2      <a href=\"http://twitter.com/download/iphone\" r...         jijitsuu   \n",
              "3      <a href=\"http://twitter.com/download/iphone\" r...         SahapunB   \n",
              "4      <a href=\"http://twitter.com/download/iphone\" r...      stella22_97   \n",
              "...                                                  ...              ...   \n",
              "14995  <a href=\"http://twitter.com/download/android\" ...        tommysboi   \n",
              "14996  <a href=\"http://twitter.com/download/iphone\" r...  kimberleywithae   \n",
              "14997  <a href=\"http://twitter.com/download/android\" ...       Gnanavel07   \n",
              "14998  <a href=\"https://mobile.twitter.com\" rel=\"nofo...       _moonljght   \n",
              "14999  <a href=\"http://twitter.com/download/android\" ...  CaterinaCabrel1   \n",
              "\n",
              "       retweetCount  isRetweet  retweeted  longitude  latitude  \n",
              "0               637       True      False        NaN       NaN  \n",
              "1               302       True      False        NaN       NaN  \n",
              "2                 0      False      False        NaN       NaN  \n",
              "3             23781       True      False        NaN       NaN  \n",
              "4             13067       True      False        NaN       NaN  \n",
              "...             ...        ...        ...        ...       ...  \n",
              "14995            33       True      False        NaN       NaN  \n",
              "14996          2307       True      False        NaN       NaN  \n",
              "14997             0      False      False        NaN       NaN  \n",
              "14998         13167       True      False        NaN       NaN  \n",
              "14999         10736       True      False        NaN       NaN  \n",
              "\n",
              "[15000 rows x 17 columns]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "tweets_df = pd.read_csv(\"tweets.csv\", encoding='latin1')\n",
        "\n",
        "tweets_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhuPusbaKVFS"
      },
      "source": [
        "#### HW 2: Create Target column from retweetCount >np.median[retweetCount] create dataset in .csv file with new features along with .yaml file with it's descritpion, median[retweetCount] value stored in retweetCount_median variable, preferably store data in data folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1755.0\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>text</th>\n",
              "      <th>favorited</th>\n",
              "      <th>favoriteCount</th>\n",
              "      <th>replyToSN</th>\n",
              "      <th>created</th>\n",
              "      <th>truncated</th>\n",
              "      <th>replyToSID</th>\n",
              "      <th>id</th>\n",
              "      <th>replyToUID</th>\n",
              "      <th>statusSource</th>\n",
              "      <th>screenName</th>\n",
              "      <th>retweetCount</th>\n",
              "      <th>isRetweet</th>\n",
              "      <th>retweeted</th>\n",
              "      <th>longitude</th>\n",
              "      <th>latitude</th>\n",
              "      <th>Target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>RT @mrvelstan: literally nobody:\\r\\nme:\\r\\n\\r\\...</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2019-04-23 10:43:30</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1120639328034676737</td>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
              "      <td>DavidAc96</td>\n",
              "      <td>637</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>RT @agntecarter: im emotional, sorry!!\\r\\n\\r\\...</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2019-04-23 10:43:30</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1120639325199196160</td>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
              "      <td>NRmalaa</td>\n",
              "      <td>302</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>saving these bingo cards for tomorrow \\r\\n©\\r\\...</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2019-04-23 10:43:30</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1120639324683292674</td>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
              "      <td>jijitsuu</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>RT @HelloBoon: Man these #AvengersEndgame ads ...</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2019-04-23 10:43:29</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1120639323328540672</td>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
              "      <td>SahapunB</td>\n",
              "      <td>23781</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>RT @Marvel: We salute you, @ChrisEvans! #Capta...</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2019-04-23 10:43:29</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1120639321571074048</td>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
              "      <td>stella22_97</td>\n",
              "      <td>13067</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14995</th>\n",
              "      <td>14996</td>\n",
              "      <td>RT @natsdany: First time                  Last...</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2019-04-23 09:22:03</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1120618828918951937</td>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
              "      <td>tommysboi</td>\n",
              "      <td>33</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14996</th>\n",
              "      <td>14997</td>\n",
              "      <td>RT @MTVNEWS: The #AvengersEndgame cast has see...</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2019-04-23 09:22:03</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1120618828038311936</td>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
              "      <td>kimberleywithae</td>\n",
              "      <td>2307</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14997</th>\n",
              "      <td>14998</td>\n",
              "      <td>@SPICinemas kindly announce the approximate ti...</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>SPICinemas</td>\n",
              "      <td>2019-04-23 09:22:02</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1120618823667920896</td>\n",
              "      <td>919079586.0</td>\n",
              "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
              "      <td>Gnanavel07</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14998</th>\n",
              "      <td>14999</td>\n",
              "      <td>RT @Marvel: We salute you, @ChrisEvans! #Capta...</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2019-04-23 09:22:02</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1120618823600803840</td>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
              "      <td>_moonljght</td>\n",
              "      <td>13167</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14999</th>\n",
              "      <td>15000</td>\n",
              "      <td>RT @Avengers: Welcome to the party, @RobertDow...</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2019-04-23 09:22:01</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1120618822300569601</td>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
              "      <td>CaterinaCabrel1</td>\n",
              "      <td>10736</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>15000 rows × 18 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Unnamed: 0                                               text  \\\n",
              "0               1  RT @mrvelstan: literally nobody:\\r\\nme:\\r\\n\\r\\...   \n",
              "1               2  RT @agntecarter: im emotional, sorry!!\\r\\n\\r\\...   \n",
              "2               3  saving these bingo cards for tomorrow \\r\\n©\\r\\...   \n",
              "3               4  RT @HelloBoon: Man these #AvengersEndgame ads ...   \n",
              "4               5  RT @Marvel: We salute you, @ChrisEvans! #Capta...   \n",
              "...           ...                                                ...   \n",
              "14995       14996  RT @natsdany: First time                  Last...   \n",
              "14996       14997  RT @MTVNEWS: The #AvengersEndgame cast has see...   \n",
              "14997       14998  @SPICinemas kindly announce the approximate ti...   \n",
              "14998       14999  RT @Marvel: We salute you, @ChrisEvans! #Capta...   \n",
              "14999       15000  RT @Avengers: Welcome to the party, @RobertDow...   \n",
              "\n",
              "       favorited  favoriteCount   replyToSN              created  truncated  \\\n",
              "0          False              0         NaN  2019-04-23 10:43:30      False   \n",
              "1          False              0         NaN  2019-04-23 10:43:30      False   \n",
              "2          False              0         NaN  2019-04-23 10:43:30      False   \n",
              "3          False              0         NaN  2019-04-23 10:43:29      False   \n",
              "4          False              0         NaN  2019-04-23 10:43:29      False   \n",
              "...          ...            ...         ...                  ...        ...   \n",
              "14995      False              0         NaN  2019-04-23 09:22:03      False   \n",
              "14996      False              0         NaN  2019-04-23 09:22:03      False   \n",
              "14997      False              0  SPICinemas  2019-04-23 09:22:02      False   \n",
              "14998      False              0         NaN  2019-04-23 09:22:02      False   \n",
              "14999      False              0         NaN  2019-04-23 09:22:01      False   \n",
              "\n",
              "       replyToSID                   id   replyToUID  \\\n",
              "0             NaN  1120639328034676737          NaN   \n",
              "1             NaN  1120639325199196160          NaN   \n",
              "2             NaN  1120639324683292674          NaN   \n",
              "3             NaN  1120639323328540672          NaN   \n",
              "4             NaN  1120639321571074048          NaN   \n",
              "...           ...                  ...          ...   \n",
              "14995         NaN  1120618828918951937          NaN   \n",
              "14996         NaN  1120618828038311936          NaN   \n",
              "14997         NaN  1120618823667920896  919079586.0   \n",
              "14998         NaN  1120618823600803840          NaN   \n",
              "14999         NaN  1120618822300569601          NaN   \n",
              "\n",
              "                                            statusSource       screenName  \\\n",
              "0      <a href=\"http://twitter.com/download/android\" ...        DavidAc96   \n",
              "1      <a href=\"http://twitter.com/download/iphone\" r...          NRmalaa   \n",
              "2      <a href=\"http://twitter.com/download/iphone\" r...         jijitsuu   \n",
              "3      <a href=\"http://twitter.com/download/iphone\" r...         SahapunB   \n",
              "4      <a href=\"http://twitter.com/download/iphone\" r...      stella22_97   \n",
              "...                                                  ...              ...   \n",
              "14995  <a href=\"http://twitter.com/download/android\" ...        tommysboi   \n",
              "14996  <a href=\"http://twitter.com/download/iphone\" r...  kimberleywithae   \n",
              "14997  <a href=\"http://twitter.com/download/android\" ...       Gnanavel07   \n",
              "14998  <a href=\"https://mobile.twitter.com\" rel=\"nofo...       _moonljght   \n",
              "14999  <a href=\"http://twitter.com/download/android\" ...  CaterinaCabrel1   \n",
              "\n",
              "       retweetCount  isRetweet  retweeted  longitude  latitude  Target  \n",
              "0               637       True      False        NaN       NaN   False  \n",
              "1               302       True      False        NaN       NaN   False  \n",
              "2                 0      False      False        NaN       NaN   False  \n",
              "3             23781       True      False        NaN       NaN    True  \n",
              "4             13067       True      False        NaN       NaN    True  \n",
              "...             ...        ...        ...        ...       ...     ...  \n",
              "14995            33       True      False        NaN       NaN   False  \n",
              "14996          2307       True      False        NaN       NaN    True  \n",
              "14997             0      False      False        NaN       NaN   False  \n",
              "14998         13167       True      False        NaN       NaN    True  \n",
              "14999         10736       True      False        NaN       NaN    True  \n",
              "\n",
              "[15000 rows x 18 columns]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tweets_with_target_df = tweets_df.copy()\n",
        "\n",
        "retweet_count_median = tweets_with_target_df['retweetCount'].median()\n",
        "\n",
        "tweets_with_target_df['Target'] = tweets_with_target_df['retweetCount'] > retweet_count_median\n",
        "\n",
        "print(retweet_count_median)\n",
        "\n",
        "tweets_with_target_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "!mkdir -p data\n",
        "\n",
        "tweets_with_target_df.to_csv('data/tweets_with_target.csv', index=False)\n",
        "\n",
        "import yaml\n",
        "\n",
        "yaml_content = {\n",
        "    'name': 'Tweets with target',\n",
        "    'description': 'Tweets with target column added',\n",
        "    'retweetCount_median': float(retweet_count_median),\n",
        "    'num_rows': len(tweets_with_target_df),\n",
        "    'num_columns': len(tweets_with_target_df.columns),\n",
        "    'columns': list(tweets_with_target_df.columns)\n",
        "}\n",
        "\n",
        "with open('data/tweets_with_target.desc.yaml', 'w') as file:\n",
        "    yaml.dump(yaml_content, file, sort_keys=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xehyn36dKVFS"
      },
      "source": [
        "#### HW 3: Сreate a folder with experiments, make it a python package, organize all transformers and classifiers wih python modules and .yaml files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "!mkdir -p experiments\n",
        "\n",
        "!touch experiments/__init__.py\n",
        "\n",
        "!mkdir -p experiments/transformers\n",
        "\n",
        "!touch experiments/transformers/__init__.py\n",
        "\n",
        "!mkdir -p experiments/classifiers\n",
        "\n",
        "!touch experiments/classifiers/__init__.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2S0J_4ZKVFT"
      },
      "source": [
        "#### HW 4: Extract names of avengers endgame characters from text\n",
        "\n",
        "Examples of tags in quetion:\n",
        "#Thanos\n",
        "#WinterSoldier\n",
        "#CaptainAmerica\n",
        "#blackwidow\n",
        "#CaptainMarvel\n",
        "#Mantis\n",
        "\n",
        "store the resulting dataset in data folder with .yaml description, preferable naming for variable names is lowercase name of the tag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>text</th>\n",
              "      <th>favorited</th>\n",
              "      <th>favoriteCount</th>\n",
              "      <th>replyToSN</th>\n",
              "      <th>created</th>\n",
              "      <th>truncated</th>\n",
              "      <th>replyToSID</th>\n",
              "      <th>id</th>\n",
              "      <th>replyToUID</th>\n",
              "      <th>...</th>\n",
              "      <th>okoye</th>\n",
              "      <th>shuri</th>\n",
              "      <th>pepperpotts</th>\n",
              "      <th>happyhogan</th>\n",
              "      <th>nickfury</th>\n",
              "      <th>mariahill</th>\n",
              "      <th>wong</th>\n",
              "      <th>hankpym</th>\n",
              "      <th>janetvandyne</th>\n",
              "      <th>wasp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>RT @mrvelstan: literally nobody:\\r\\nme:\\r\\n\\r\\...</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2019-04-23 10:43:30</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1120639328034676737</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>RT @agntecarter: im emotional, sorry!!\\r\\n\\r\\...</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2019-04-23 10:43:30</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1120639325199196160</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>saving these bingo cards for tomorrow \\r\\n©\\r\\...</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2019-04-23 10:43:30</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1120639324683292674</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>RT @HelloBoon: Man these #AvengersEndgame ads ...</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2019-04-23 10:43:29</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1120639323328540672</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>RT @Marvel: We salute you, @ChrisEvans! #Capta...</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2019-04-23 10:43:29</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1120639321571074048</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14995</th>\n",
              "      <td>14996</td>\n",
              "      <td>RT @natsdany: First time                  Last...</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2019-04-23 09:22:03</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1120618828918951937</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14996</th>\n",
              "      <td>14997</td>\n",
              "      <td>RT @MTVNEWS: The #AvengersEndgame cast has see...</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2019-04-23 09:22:03</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1120618828038311936</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14997</th>\n",
              "      <td>14998</td>\n",
              "      <td>@SPICinemas kindly announce the approximate ti...</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>SPICinemas</td>\n",
              "      <td>2019-04-23 09:22:02</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1120618823667920896</td>\n",
              "      <td>919079586.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14998</th>\n",
              "      <td>14999</td>\n",
              "      <td>RT @Marvel: We salute you, @ChrisEvans! #Capta...</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2019-04-23 09:22:02</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1120618823600803840</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14999</th>\n",
              "      <td>15000</td>\n",
              "      <td>RT @Avengers: Welcome to the party, @RobertDow...</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2019-04-23 09:22:01</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1120618822300569601</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>15000 rows × 52 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Unnamed: 0                                               text  \\\n",
              "0               1  RT @mrvelstan: literally nobody:\\r\\nme:\\r\\n\\r\\...   \n",
              "1               2  RT @agntecarter: im emotional, sorry!!\\r\\n\\r\\...   \n",
              "2               3  saving these bingo cards for tomorrow \\r\\n©\\r\\...   \n",
              "3               4  RT @HelloBoon: Man these #AvengersEndgame ads ...   \n",
              "4               5  RT @Marvel: We salute you, @ChrisEvans! #Capta...   \n",
              "...           ...                                                ...   \n",
              "14995       14996  RT @natsdany: First time                  Last...   \n",
              "14996       14997  RT @MTVNEWS: The #AvengersEndgame cast has see...   \n",
              "14997       14998  @SPICinemas kindly announce the approximate ti...   \n",
              "14998       14999  RT @Marvel: We salute you, @ChrisEvans! #Capta...   \n",
              "14999       15000  RT @Avengers: Welcome to the party, @RobertDow...   \n",
              "\n",
              "       favorited  favoriteCount   replyToSN              created  truncated  \\\n",
              "0          False              0         NaN  2019-04-23 10:43:30      False   \n",
              "1          False              0         NaN  2019-04-23 10:43:30      False   \n",
              "2          False              0         NaN  2019-04-23 10:43:30      False   \n",
              "3          False              0         NaN  2019-04-23 10:43:29      False   \n",
              "4          False              0         NaN  2019-04-23 10:43:29      False   \n",
              "...          ...            ...         ...                  ...        ...   \n",
              "14995      False              0         NaN  2019-04-23 09:22:03      False   \n",
              "14996      False              0         NaN  2019-04-23 09:22:03      False   \n",
              "14997      False              0  SPICinemas  2019-04-23 09:22:02      False   \n",
              "14998      False              0         NaN  2019-04-23 09:22:02      False   \n",
              "14999      False              0         NaN  2019-04-23 09:22:01      False   \n",
              "\n",
              "       replyToSID                   id   replyToUID  ... okoye shuri  \\\n",
              "0             NaN  1120639328034676737          NaN  ...     0     0   \n",
              "1             NaN  1120639325199196160          NaN  ...     0     0   \n",
              "2             NaN  1120639324683292674          NaN  ...     0     0   \n",
              "3             NaN  1120639323328540672          NaN  ...     0     0   \n",
              "4             NaN  1120639321571074048          NaN  ...     0     0   \n",
              "...           ...                  ...          ...  ...   ...   ...   \n",
              "14995         NaN  1120618828918951937          NaN  ...     0     0   \n",
              "14996         NaN  1120618828038311936          NaN  ...     0     0   \n",
              "14997         NaN  1120618823667920896  919079586.0  ...     0     0   \n",
              "14998         NaN  1120618823600803840          NaN  ...     0     0   \n",
              "14999         NaN  1120618822300569601          NaN  ...     0     0   \n",
              "\n",
              "       pepperpotts  happyhogan  nickfury  mariahill  wong  hankpym  \\\n",
              "0                0           0         0          0     0        0   \n",
              "1                0           0         0          0     0        0   \n",
              "2                0           0         0          0     0        0   \n",
              "3                0           0         0          0     0        0   \n",
              "4                0           0         0          0     0        0   \n",
              "...            ...         ...       ...        ...   ...      ...   \n",
              "14995            0           0         0          0     0        0   \n",
              "14996            0           0         0          0     0        0   \n",
              "14997            0           0         0          0     0        0   \n",
              "14998            0           0         0          0     0        0   \n",
              "14999            0           0         0          0     0        0   \n",
              "\n",
              "       janetvandyne  wasp  \n",
              "0                 0     0  \n",
              "1                 0     0  \n",
              "2                 0     0  \n",
              "3                 0     0  \n",
              "4                 0     0  \n",
              "...             ...   ...  \n",
              "14995             0     0  \n",
              "14996             0     0  \n",
              "14997             0     0  \n",
              "14998             0     0  \n",
              "14999             0     0  \n",
              "\n",
              "[15000 rows x 52 columns]"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tweets_with_characters_and_target_df = tweets_with_target_df.copy()\n",
        "\n",
        "characters = [\n",
        "    \"ironman\", \"captainamerica\", \"hulk\", \"thor\", \"blackwidow\", \"hawkeye\",\n",
        "    \"thanos\", \"antman\", \"captainmarvel\", \"spiderman\", \"doctorstrange\",\n",
        "    \"blackpanther\", \"nebula\", \"gamora\", \"loki\", \"scarletwitch\", \"vision\",\n",
        "    \"falcon\", \"wintersoldier\", \"starlord\", \"drax\", \"groot\", \"rocket\",\n",
        "    \"mantis\", \"okoye\", \"shuri\", \"pepperpotts\", \"happyhogan\", \"nickfury\",\n",
        "    \"mariahill\", \"wong\", \"hankpym\", \"janetvandyne\", \"wasp\"\n",
        "]\n",
        "\n",
        "for character in characters:\n",
        "    tweets_with_characters_and_target_df[character] = tweets_with_characters_and_target_df['text'].apply(lambda x: 1 if character in x.replace(\" \", \"\").replace(\"\\t\", \"\").replace(\"\\n\", \"\").lower() else 0)\n",
        "\n",
        "tweets_with_characters_and_target_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "!mkdir -p data\n",
        "\n",
        "tweets_with_characters_and_target_df.to_csv('data/tweets_with_characters_and_target.csv', index=False)\n",
        "\n",
        "import yaml\n",
        "\n",
        "yaml_content = {\n",
        "    'name': 'Tweets with characters and target',\n",
        "    'description': 'Tweets with characters and target columns added',\n",
        "    'num_rows': len(tweets_with_characters_and_target_df),\n",
        "    'num_columns': len(tweets_with_characters_and_target_df.columns),\n",
        "    'columns': list(tweets_with_characters_and_target_df.columns)\n",
        "}\n",
        "\n",
        "with open('data/tweets_with_characters_and_target.desc.yaml', 'w') as file:\n",
        "    yaml.dump(yaml_content, file, sort_keys=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBMtIHiiKVFT"
      },
      "source": [
        "#### HW 5: Create set of classifiers for linear regression with different features for feature generation.\n",
        "use the following methods of feature generation\n",
        "\n",
        " - word tokenization with 1-gram, f\n",
        " - pos tokenization with extraction of all NP\n",
        " - pos tokenization with 2-gram bag of tokens for all NNP\n",
        " - 2-gram bag of characters for screenName features\n",
        " - polynomial features for presence of avenger character names\n",
        "\n",
        "Organize classes of transformers in various .py files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting experiments/transformers/word_vectorizer_1gram.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile experiments/transformers/word_vectorizer_1gram.py\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class WordVectorizer1Gram(BaseEstimator, TransformerMixin):\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.vectorizer = CountVectorizer(ngram_range=(1, 1))\n",
        "    \n",
        "    def fit(self, dataset, y=None):\n",
        "        self.vectorizer.fit(dataset['text'])\n",
        "        return self\n",
        "    \n",
        "    def transform(self, dataset):\n",
        "        return self.vectorizer.transform(dataset['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: spacy in /home/brayand/.local/lib/python3.10/site-packages (3.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/brayand/.local/lib/python3.10/site-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: jinja2 in /home/brayand/.local/lib/python3.10/site-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/brayand/.local/lib/python3.10/site-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /home/brayand/.local/lib/python3.10/site-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/brayand/.local/lib/python3.10/site-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/brayand/.local/lib/python3.10/site-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /home/brayand/.local/lib/python3.10/site-packages (from spacy) (0.3.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/brayand/.local/lib/python3.10/site-packages (from spacy) (2.5.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/brayand/.local/lib/python3.10/site-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/brayand/.local/lib/python3.10/site-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /home/brayand/.local/lib/python3.10/site-packages (from spacy) (1.23.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/brayand/.local/lib/python3.10/site-packages (from spacy) (4.65.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/brayand/.local/lib/python3.10/site-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/brayand/.local/lib/python3.10/site-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/brayand/.local/lib/python3.10/site-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/brayand/.local/lib/python3.10/site-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: setuptools in /home/brayand/.local/lib/python3.10/site-packages (from spacy) (67.8.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/brayand/.local/lib/python3.10/site-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/brayand/.local/lib/python3.10/site-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /home/brayand/.local/lib/python3.10/site-packages (from spacy) (8.2.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/lib/python3/dist-packages (from packaging>=20.0->spacy) (2.4.7)\n",
            "Requirement already satisfied: pydantic-core==2.14.5 in /home/brayand/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.14.5)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /home/brayand/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /home/brayand/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/brayand/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/brayand/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.5.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.5)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/brayand/.local/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/brayand/.local/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/brayand/.local/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.3)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /home/brayand/.local/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/brayand/.local/lib/python3.10/site-packages (from jinja2->spacy) (2.1.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m434.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /home/brayand/.local/lib/python3.10/site-packages (from en-core-web-sm==3.7.1) (3.7.2)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /home/brayand/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/brayand/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/brayand/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/brayand/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/brayand/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/brayand/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/brayand/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/brayand/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/brayand/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.2)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /home/brayand/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.23.5)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /home/brayand/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/brayand/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/brayand/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.65.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/brayand/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/brayand/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (21.3)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /home/brayand/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
            "Requirement already satisfied: jinja2 in /home/brayand/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/brayand/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/brayand/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
            "Requirement already satisfied: setuptools in /home/brayand/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (67.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/lib/python3/dist-packages (from packaging>=20.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.7)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /home/brayand/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /home/brayand/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.9.0)\n",
            "Requirement already satisfied: pydantic-core==2.14.5 in /home/brayand/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.14.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/brayand/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2023.5.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/brayand/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.5)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/brayand/.local/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/brayand/.local/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/brayand/.local/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.3)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /home/brayand/.local/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/brayand/.local/lib/python3.10/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "%pip install spacy\n",
        "!python3 -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting experiments/transformers/np_vectorizer.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile experiments/transformers/np_vectorizer.py\n",
        "\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class NPVectorizer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
        "        self.vectorizer = CountVectorizer(ngram_range=(1, 1))\n",
        "\n",
        "    def fit(self, dataset, y=None):\n",
        "        noun_phrases = []\n",
        "        for text in dataset['text']:\n",
        "            doc = self.nlp(text)\n",
        "            phrases = [chunk.text for chunk in doc.noun_chunks]\n",
        "            noun_phrases.append(\" \".join(phrases))\n",
        "        self.vectorizer.fit(noun_phrases)\n",
        "        return self\n",
        "\n",
        "    def transform(self, dataset):\n",
        "        noun_phrases = []\n",
        "        for text in dataset['text']:\n",
        "            doc = self.nlp(text)\n",
        "            phrases = [chunk.text for chunk in doc.noun_chunks]\n",
        "            noun_phrases.append(\" \".join(phrases))\n",
        "\n",
        "        return self.vectorizer.transform(noun_phrases)\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting experiments/transformers/word_vectorizer_2gram.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile experiments/transformers/word_vectorizer_2gram.py\n",
        "\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class WordVectorizer2Gram(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
        "        self.vectorizer = CountVectorizer(tokenizer=self.custom_tokenizer, ngram_range=(2, 2))\n",
        "\n",
        "    def custom_tokenizer(self, text):\n",
        "        doc = self.nlp(text)\n",
        "        nnps = [token.text for token in doc if token.pos_ == 'PROPN']\n",
        "        return nnps\n",
        "\n",
        "    def fit(self, dataset, y=None):\n",
        "        self.vectorizer.fit(dataset['text'])\n",
        "        return self\n",
        "\n",
        "    def transform(self, dataset):\n",
        "        return self.vectorizer.transform(dataset['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting experiments/transformers/word_vectorizer_2gram_by_character.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile experiments/transformers/word_vectorizer_2gram_by_character.py\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class WordVectorizer2GramByCharacter(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        self.vectorizer = CountVectorizer(analyzer='char', ngram_range=(2, 2))\n",
        "    \n",
        "    def fit(self, dataset, y=None):\n",
        "        self.vectorizer.fit(dataset['screenName'])\n",
        "        return self\n",
        "\n",
        "    def transform(self, dataset):\n",
        "        return self.vectorizer.transform(dataset['screenName'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting experiments/transformers/polynomial_features_by_character.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile experiments/transformers/polynomial_features_by_character.py\n",
        "\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class PolynomialFeaturesByCharacter(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        self.characters = [\n",
        "            \"ironman\", \"captainamerica\", \"hulk\", \"thor\", \"blackwidow\", \"hawkeye\",\n",
        "            \"thanos\", \"antman\", \"captainmarvel\", \"spiderman\", \"doctorstrange\",\n",
        "            \"blackpanther\", \"nebula\", \"gamora\", \"loki\", \"scarletwitch\", \"vision\",\n",
        "            \"falcon\", \"wintersoldier\", \"starlord\", \"drax\", \"groot\", \"rocket\",\n",
        "            \"mantis\", \"okoye\", \"shuri\", \"pepperpotts\", \"happyhogan\", \"nickfury\",\n",
        "            \"mariahill\", \"wong\", \"hankpym\", \"janetvandyne\", \"wasp\"\n",
        "        ]\n",
        "        self.poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
        "\n",
        "    def fit(self, dataset, y=None):\n",
        "        self.poly.fit(dataset[self.characters])\n",
        "        return self\n",
        "\n",
        "    def transform(self, dataset):\n",
        "        return self.poly.transform(dataset[self.characters])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(100, 545)\n",
            "(100, 329)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/brayand/.local/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(100, 149)\n",
            "(100, 373)\n",
            "(100, 595)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>text</th>\n",
              "      <th>favorited</th>\n",
              "      <th>favoriteCount</th>\n",
              "      <th>replyToSN</th>\n",
              "      <th>created</th>\n",
              "      <th>truncated</th>\n",
              "      <th>replyToSID</th>\n",
              "      <th>id</th>\n",
              "      <th>replyToUID</th>\n",
              "      <th>...</th>\n",
              "      <th>okoye</th>\n",
              "      <th>shuri</th>\n",
              "      <th>pepperpotts</th>\n",
              "      <th>happyhogan</th>\n",
              "      <th>nickfury</th>\n",
              "      <th>mariahill</th>\n",
              "      <th>wong</th>\n",
              "      <th>hankpym</th>\n",
              "      <th>janetvandyne</th>\n",
              "      <th>wasp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>RT @mrvelstan: literally nobody:\\r\\nme:\\r\\n\\r\\...</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2019-04-23 10:43:30</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1120639328034676737</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>RT @agntecarter: im emotional, sorry!!\\r\\n\\r\\...</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2019-04-23 10:43:30</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1120639325199196160</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>saving these bingo cards for tomorrow \\r\\n©\\r\\...</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2019-04-23 10:43:30</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1120639324683292674</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>RT @HelloBoon: Man these #AvengersEndgame ads ...</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2019-04-23 10:43:29</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1120639323328540672</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>RT @Marvel: We salute you, @ChrisEvans! #Capta...</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2019-04-23 10:43:29</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1120639321571074048</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>96</td>\n",
              "      <td>RT @slashfilm: Imagine the best possible versi...</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2019-04-23 10:42:57</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1120639188410486784</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>97</td>\n",
              "      <td>RT @Marvel: .@letitiawright works the red carp...</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2019-04-23 10:42:57</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1120639187689058305</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>98</td>\n",
              "      <td>RT @cprattsource: [ &lt;U+0001F3A5&gt; ] #video  @e...</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2019-04-23 10:42:56</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1120639184316710914</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>99</td>\n",
              "      <td>RT @Avengers: .@AnthonyMackie soars onto the c...</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2019-04-23 10:42:56</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1120639182744051713</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>100</td>\n",
              "      <td>RT @Avengers: We salute you, @ChrisEvans! #Cap...</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2019-04-23 10:42:56</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1120639182320222208</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 52 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    Unnamed: 0                                               text  favorited  \\\n",
              "0            1  RT @mrvelstan: literally nobody:\\r\\nme:\\r\\n\\r\\...      False   \n",
              "1            2  RT @agntecarter: im emotional, sorry!!\\r\\n\\r\\...      False   \n",
              "2            3  saving these bingo cards for tomorrow \\r\\n©\\r\\...      False   \n",
              "3            4  RT @HelloBoon: Man these #AvengersEndgame ads ...      False   \n",
              "4            5  RT @Marvel: We salute you, @ChrisEvans! #Capta...      False   \n",
              "..         ...                                                ...        ...   \n",
              "95          96  RT @slashfilm: Imagine the best possible versi...      False   \n",
              "96          97  RT @Marvel: .@letitiawright works the red carp...      False   \n",
              "97          98  RT @cprattsource: [ <U+0001F3A5> ] #video  @e...      False   \n",
              "98          99  RT @Avengers: .@AnthonyMackie soars onto the c...      False   \n",
              "99         100  RT @Avengers: We salute you, @ChrisEvans! #Cap...      False   \n",
              "\n",
              "    favoriteCount replyToSN              created  truncated  replyToSID  \\\n",
              "0               0       NaN  2019-04-23 10:43:30      False         NaN   \n",
              "1               0       NaN  2019-04-23 10:43:30      False         NaN   \n",
              "2               0       NaN  2019-04-23 10:43:30      False         NaN   \n",
              "3               0       NaN  2019-04-23 10:43:29      False         NaN   \n",
              "4               0       NaN  2019-04-23 10:43:29      False         NaN   \n",
              "..            ...       ...                  ...        ...         ...   \n",
              "95              0       NaN  2019-04-23 10:42:57      False         NaN   \n",
              "96              0       NaN  2019-04-23 10:42:57      False         NaN   \n",
              "97              0       NaN  2019-04-23 10:42:56      False         NaN   \n",
              "98              0       NaN  2019-04-23 10:42:56      False         NaN   \n",
              "99              0       NaN  2019-04-23 10:42:56      False         NaN   \n",
              "\n",
              "                     id  replyToUID  ... okoye shuri  pepperpotts  happyhogan  \\\n",
              "0   1120639328034676737         NaN  ...     0     0            0           0   \n",
              "1   1120639325199196160         NaN  ...     0     0            0           0   \n",
              "2   1120639324683292674         NaN  ...     0     0            0           0   \n",
              "3   1120639323328540672         NaN  ...     0     0            0           0   \n",
              "4   1120639321571074048         NaN  ...     0     0            0           0   \n",
              "..                  ...         ...  ...   ...   ...          ...         ...   \n",
              "95  1120639188410486784         NaN  ...     0     0            0           0   \n",
              "96  1120639187689058305         NaN  ...     0     1            0           0   \n",
              "97  1120639184316710914         NaN  ...     0     0            0           0   \n",
              "98  1120639182744051713         NaN  ...     0     0            0           0   \n",
              "99  1120639182320222208         NaN  ...     0     0            0           0   \n",
              "\n",
              "    nickfury  mariahill  wong  hankpym  janetvandyne  wasp  \n",
              "0          0          0     0        0             0     0  \n",
              "1          0          0     0        0             0     0  \n",
              "2          0          0     0        0             0     0  \n",
              "3          0          0     0        0             0     0  \n",
              "4          0          0     0        0             0     0  \n",
              "..       ...        ...   ...      ...           ...   ...  \n",
              "95         0          0     0        0             0     0  \n",
              "96         0          0     0        0             0     0  \n",
              "97         0          0     0        0             0     0  \n",
              "98         0          0     0        0             0     0  \n",
              "99         0          0     0        0             0     0  \n",
              "\n",
              "[100 rows x 52 columns]"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from importlib import reload\n",
        "\n",
        "import experiments.transformers.word_vectorizer_1gram\n",
        "reload(experiments.transformers.word_vectorizer_1gram)\n",
        "\n",
        "import experiments.transformers.np_vectorizer\n",
        "reload(experiments.transformers.np_vectorizer)\n",
        "\n",
        "import experiments.transformers.word_vectorizer_2gram\n",
        "reload(experiments.transformers.word_vectorizer_2gram)\n",
        "\n",
        "import experiments.transformers.word_vectorizer_2gram_by_character\n",
        "reload(experiments.transformers.word_vectorizer_2gram_by_character)\n",
        "\n",
        "import experiments.transformers.polynomial_features_by_character\n",
        "reload(experiments.transformers.polynomial_features_by_character)\n",
        "\n",
        "# I am using only the first 100 rows for the experiments.\n",
        "reduced_df = tweets_with_characters_and_target_df.copy().iloc[0:100]\n",
        "\n",
        "word_vectorizer_1gram = experiments.transformers.word_vectorizer_1gram.WordVectorizer1Gram()\n",
        "np_vectorizer = experiments.transformers.np_vectorizer.NPVectorizer()\n",
        "word_vectorizer_2gram = experiments.transformers.word_vectorizer_2gram.WordVectorizer2Gram()\n",
        "word_vectorizer_2gram_by_character = experiments.transformers.word_vectorizer_2gram_by_character.WordVectorizer2GramByCharacter()\n",
        "polynomial_features_by_character = experiments.transformers.polynomial_features_by_character.PolynomialFeaturesByCharacter()\n",
        "\n",
        "print(word_vectorizer_1gram.fit_transform(reduced_df).shape)\n",
        "print(np_vectorizer.fit_transform(reduced_df).shape)\n",
        "print(word_vectorizer_2gram.fit_transform(reduced_df).shape)\n",
        "print(word_vectorizer_2gram_by_character.fit_transform(reduced_df).shape)\n",
        "print(polynomial_features_by_character.fit_transform(reduced_df).shape)\n",
        "\n",
        "reduced_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gE-NT52DKVFT"
      },
      "source": [
        "#### HW 6: Apply PCA dimensionality reduction and LogisticRegression to predict Target, construct pipelines for all transformers from HW 5 (so it will be 5 various pipeline, name them exp_hw6_1, ... exp_hw6_5) implement them as custom classifiers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 213,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting experiments/classifiers/exp_hw6_1.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile experiments/classifiers/exp_hw6_1.py\n",
        "\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from importlib import reload\n",
        "import experiments.transformers.word_vectorizer_1gram\n",
        "reload(experiments.transformers.word_vectorizer_1gram)\n",
        "\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "\n",
        "class ExpHW6Classifier1(BaseEstimator, ClassifierMixin):\n",
        "\n",
        "    def __init__(self, n_components=2, extra_features=[]):\n",
        "        self.transformer = Pipeline([\n",
        "            ('word_vectorizer_1gram', experiments.transformers.word_vectorizer_1gram.WordVectorizer1Gram()),\n",
        "            ('truncated_svd', TruncatedSVD(n_components=n_components)),\n",
        "        ])\n",
        "\n",
        "        def append_extra_features(X, extra_features):\n",
        "            return np.hstack((X, extra_features)) if extra_features is not None else X\n",
        "\n",
        "        self.classifier = Pipeline([\n",
        "            ('feature_combiner', FunctionTransformer(append_extra_features, validate=False)),\n",
        "            ('logistic_regression', LogisticRegression())\n",
        "        ])\n",
        "\n",
        "    def fit(self, dataset, y=None, extra_features=None):\n",
        "        X_transformed = self.transformer.fit_transform(dataset)\n",
        "        self.classifier.set_params(feature_combiner__kw_args={'extra_features': extra_features})\n",
        "        self.classifier.fit(X_transformed, y)\n",
        "        return self\n",
        "\n",
        "    def predict(self, dataset, extra_features=None):\n",
        "        X_transformed = self.transformer.transform(dataset)\n",
        "        self.classifier.set_params(feature_combiner__kw_args={'extra_features': extra_features})\n",
        "        return self.classifier.predict(X_transformed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 214,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting experiments/classifiers/exp_hw6_2.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile experiments/classifiers/exp_hw6_2.py\n",
        "\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from importlib import reload\n",
        "import experiments.transformers.np_vectorizer\n",
        "reload(experiments.transformers.np_vectorizer)\n",
        "\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "\n",
        "class ExpHW6Classifier2(BaseEstimator, ClassifierMixin):\n",
        "\n",
        "    def __init__(self, n_components=2, extra_features=[]):\n",
        "        self.transformer = Pipeline([\n",
        "            ('np_vectorizer', experiments.transformers.np_vectorizer.NPVectorizer()),\n",
        "            ('truncated_svd', TruncatedSVD(n_components=n_components)),\n",
        "        ])\n",
        "\n",
        "        def append_extra_features(X, extra_features):\n",
        "            return np.hstack((X, extra_features)) if extra_features is not None else X\n",
        "\n",
        "        self.classifier = Pipeline([\n",
        "            ('feature_combiner', FunctionTransformer(append_extra_features, validate=False)),\n",
        "            ('logistic_regression', LogisticRegression())\n",
        "        ])\n",
        "\n",
        "    def fit(self, dataset, y=None, extra_features=None):\n",
        "        X_transformed = self.transformer.fit_transform(dataset)\n",
        "        self.classifier.set_params(feature_combiner__kw_args={'extra_features': extra_features})\n",
        "        self.classifier.fit(X_transformed, y)\n",
        "        return self\n",
        "\n",
        "    def predict(self, dataset, extra_features=None):\n",
        "        X_transformed = self.transformer.transform(dataset)\n",
        "        self.classifier.set_params(feature_combiner__kw_args={'extra_features': extra_features})\n",
        "        return self.classifier.predict(X_transformed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 215,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting experiments/classifiers/exp_hw6_3.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile experiments/classifiers/exp_hw6_3.py\n",
        "\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from importlib import reload\n",
        "import experiments.transformers.word_vectorizer_2gram\n",
        "reload(experiments.transformers.word_vectorizer_2gram)\n",
        "\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "\n",
        "class ExpHW6Classifier3(BaseEstimator, ClassifierMixin):\n",
        "\n",
        "    def __init__(self, n_components=2, extra_features=[]):\n",
        "        self.transformer = Pipeline([\n",
        "            ('word_vectorizer_2gram', experiments.transformers.word_vectorizer_2gram.WordVectorizer2Gram()),\n",
        "            ('truncated_svd', TruncatedSVD(n_components=n_components)),\n",
        "        ])\n",
        "\n",
        "        def append_extra_features(X, extra_features):\n",
        "            return np.hstack((X, extra_features)) if extra_features is not None else X\n",
        "\n",
        "        self.classifier = Pipeline([\n",
        "            ('feature_combiner', FunctionTransformer(append_extra_features, validate=False)),\n",
        "            ('logistic_regression', LogisticRegression())\n",
        "        ])\n",
        "\n",
        "    def fit(self, dataset, y=None, extra_features=None):\n",
        "        X_transformed = self.transformer.fit_transform(dataset)\n",
        "        self.classifier.set_params(feature_combiner__kw_args={'extra_features': extra_features})\n",
        "        self.classifier.fit(X_transformed, y)\n",
        "        return self\n",
        "\n",
        "    def predict(self, dataset, extra_features=None):\n",
        "        X_transformed = self.transformer.transform(dataset)\n",
        "        self.classifier.set_params(feature_combiner__kw_args={'extra_features': extra_features})\n",
        "        return self.classifier.predict(X_transformed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 216,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting experiments/classifiers/exp_hw6_4.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile experiments/classifiers/exp_hw6_4.py\n",
        "\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from importlib import reload\n",
        "import experiments.transformers.word_vectorizer_2gram_by_character\n",
        "reload(experiments.transformers.word_vectorizer_2gram_by_character)\n",
        "\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "\n",
        "class ExpHW6Classifier4(BaseEstimator, ClassifierMixin):\n",
        "\n",
        "    def __init__(self, n_components=2, extra_features=[]):\n",
        "        self.transformer = Pipeline([\n",
        "            ('word_vectorizer_2gram_by_character', experiments.transformers.word_vectorizer_2gram_by_character.WordVectorizer2GramByCharacter()),\n",
        "            ('truncated_svd', TruncatedSVD(n_components=n_components)),\n",
        "        ])\n",
        "\n",
        "        def append_extra_features(X, extra_features):\n",
        "            return np.hstack((X, extra_features)) if extra_features is not None else X\n",
        "\n",
        "        self.classifier = Pipeline([\n",
        "            ('feature_combiner', FunctionTransformer(append_extra_features, validate=False)),\n",
        "            ('logistic_regression', LogisticRegression())\n",
        "        ])\n",
        "\n",
        "    def fit(self, dataset, y=None, extra_features=None):\n",
        "        X_transformed = self.transformer.fit_transform(dataset)\n",
        "        self.classifier.set_params(feature_combiner__kw_args={'extra_features': extra_features})\n",
        "        self.classifier.fit(X_transformed, y)\n",
        "        return self\n",
        "\n",
        "    def predict(self, dataset, extra_features=None):\n",
        "        X_transformed = self.transformer.transform(dataset)\n",
        "        self.classifier.set_params(feature_combiner__kw_args={'extra_features': extra_features})\n",
        "        return self.classifier.predict(X_transformed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 217,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting experiments/classifiers/exp_hw6_5.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile experiments/classifiers/exp_hw6_5.py\n",
        "\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from importlib import reload\n",
        "import experiments.transformers.polynomial_features_by_character\n",
        "reload(experiments.transformers.polynomial_features_by_character)\n",
        "\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "\n",
        "class ExpHW6Classifier5(BaseEstimator, ClassifierMixin):\n",
        "\n",
        "    def __init__(self, n_components=2, extra_features=[]):\n",
        "        self.transformer = Pipeline([\n",
        "            ('polynomial_features_by_character', experiments.transformers.polynomial_features_by_character.PolynomialFeaturesByCharacter()),\n",
        "            ('truncated_svd', TruncatedSVD(n_components=n_components)),\n",
        "        ])\n",
        "\n",
        "        def append_extra_features(X, extra_features):\n",
        "            return np.hstack((X, extra_features)) if extra_features is not None else X\n",
        "\n",
        "        self.classifier = Pipeline([\n",
        "            ('feature_combiner', FunctionTransformer(append_extra_features, validate=False)),\n",
        "            ('logistic_regression', LogisticRegression())\n",
        "        ])\n",
        "\n",
        "    def fit(self, dataset, y=None, extra_features=None):\n",
        "        X_transformed = self.transformer.fit_transform(dataset)\n",
        "        self.classifier.set_params(feature_combiner__kw_args={'extra_features': extra_features})\n",
        "        self.classifier.fit(X_transformed, y)\n",
        "        return self\n",
        "\n",
        "    def predict(self, dataset, extra_features=None):\n",
        "        X_transformed = self.transformer.transform(dataset)\n",
        "        self.classifier.set_params(feature_combiner__kw_args={'extra_features': extra_features})\n",
        "        return self.classifier.predict(X_transformed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qNn8vkBKVFU"
      },
      "source": [
        "#### HW 7: Split dataset to train and test (it is up to you which features you will include in it) and store it data folder along with .yaml description"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_df, test_df = train_test_split(tweets_with_characters_and_target_df, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [],
      "source": [
        "!mkdir -p data\n",
        "\n",
        "train_df.to_csv('data/train_dataset.csv', index=False)\n",
        "\n",
        "import yaml\n",
        "\n",
        "yaml_content = {\n",
        "    'name': 'Train dataset',\n",
        "    'description': 'Train dataset',\n",
        "    'num_rows': len(train_df),\n",
        "    'num_columns': len(train_df.columns),\n",
        "    'columns': list(train_df.columns)\n",
        "}\n",
        "\n",
        "with open('data/train_dataset.desc.yaml', 'w') as file:\n",
        "    yaml.dump(yaml_content, file, sort_keys=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [],
      "source": [
        "!mkdir -p data\n",
        "\n",
        "test_df.to_csv('data/test_dataset.csv', index=False)\n",
        "\n",
        "import yaml\n",
        "\n",
        "yaml_content = {\n",
        "    'name': 'Test dataset',\n",
        "    'description': 'Test dataset',\n",
        "    'num_rows': len(test_df),\n",
        "    'num_columns': len(test_df.columns),\n",
        "    'columns': list(test_df.columns)\n",
        "}\n",
        "\n",
        "with open('data/test_dataset.desc.yaml', 'w') as file:\n",
        "    yaml.dump(yaml_content, file, sort_keys=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7vDCZz8KVFU"
      },
      "source": [
        "#### HW 8: Add intialization from .yaml descriptions of classifiers to implementations of classifiers at HW 6."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [],
      "source": [
        "!mkdir -p experiments/classifiers\n",
        "\n",
        "import yaml\n",
        "\n",
        "yaml_content = {\n",
        "    'name': 'Experiment 1',\n",
        "    'description': 'This is the first experiment',\n",
        "    'pca_n_components': 10,\n",
        "}\n",
        "\n",
        "with open('experiments/classifiers/exp_hw6_1.desc.yaml', 'w') as file:\n",
        "    yaml.dump(yaml_content, file, sort_keys=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [],
      "source": [
        "!mkdir -p experiments/classifiers\n",
        "\n",
        "import yaml\n",
        "\n",
        "yaml_content = {\n",
        "    'name': 'Experiment 2',\n",
        "    'description': 'This is the second experiment',\n",
        "    'pca_n_components': 20,\n",
        "}\n",
        "\n",
        "with open('experiments/classifiers/exp_hw6_2.desc.yaml', 'w') as file:\n",
        "    yaml.dump(yaml_content, file, sort_keys=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [],
      "source": [
        "!mkdir -p experiments/classifiers\n",
        "\n",
        "import yaml\n",
        "\n",
        "yaml_content = {\n",
        "    'name': 'Experiment 3',\n",
        "    'description': 'This is the third experiment',\n",
        "    'pca_n_components': 5,\n",
        "}\n",
        "\n",
        "with open('experiments/classifiers/exp_hw6_3.desc.yaml', 'w') as file:\n",
        "    yaml.dump(yaml_content, file, sort_keys=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [],
      "source": [
        "!mkdir -p experiments/classifiers\n",
        "\n",
        "import yaml\n",
        "\n",
        "yaml_content = {\n",
        "    'name': 'Experiment 4',\n",
        "    'description': 'This is the fourth experiment',\n",
        "    'pca_n_components': 10,\n",
        "}\n",
        "\n",
        "with open('experiments/classifiers/exp_hw6_4.desc.yaml', 'w') as file:\n",
        "    yaml.dump(yaml_content, file, sort_keys=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [],
      "source": [
        "!mkdir -p experiments/classifiers\n",
        "\n",
        "import yaml\n",
        "\n",
        "yaml_content = {\n",
        "    'name': 'Experiment 5',\n",
        "    'description': 'This is the fifth experiment',\n",
        "    'pca_n_components': 15,\n",
        "}\n",
        "\n",
        "with open('experiments/classifiers/exp_hw6_5.desc.yaml', 'w') as file:\n",
        "    yaml.dump(yaml_content, file, sort_keys=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3VPXJzHKVFU"
      },
      "source": [
        "#### HW 9: Train classifiers with various PCA dimensionality, bag of words and polynomial paramters paramteres on train, test them on test and store in .yaml files for every experiment with resulting metrics\n",
        "\n",
        "- accuracy\n",
        "- precision\n",
        "- recall\n",
        "- adjusted_mutual_information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# I am using only the first 1000 rows for the experiments.\n",
        "reduced_train_df, reduced_test_df = train_test_split(tweets_with_characters_and_target_df.iloc[0:1000], test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [],
      "source": [
        "!mkdir -p data\n",
        "\n",
        "reduced_train_df.to_csv('data/reduced_train_dataset.csv', index=False)\n",
        "\n",
        "import yaml\n",
        "\n",
        "yaml_content = {\n",
        "    'name': 'Reduced Train dataset',\n",
        "    'description': 'Reduced Train dataset',\n",
        "    'num_rows': len(train_df),\n",
        "    'num_columns': len(train_df.columns),\n",
        "    'columns': list(train_df.columns)\n",
        "}\n",
        "\n",
        "with open('data/reduced_train_dataset.desc.yaml', 'w') as file:\n",
        "    yaml.dump(yaml_content, file, sort_keys=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [],
      "source": [
        "!mkdir -p data\n",
        "\n",
        "reduced_test_df.to_csv('data/reduced_test_dataset.csv', index=False)\n",
        "\n",
        "import yaml\n",
        "\n",
        "yaml_content = {\n",
        "    'name': 'Reduced dataset',\n",
        "    'description': 'Reduced dataset',\n",
        "    'num_rows': len(test_df),\n",
        "    'num_columns': len(test_df.columns),\n",
        "    'columns': list(test_df.columns)\n",
        "}\n",
        "\n",
        "with open('data/reduced_test_dataset.desc.yaml', 'w') as file:\n",
        "    yaml.dump(yaml_content, file, sort_keys=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 218,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/brayand/.local/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment 1:\n",
            "Accuracy: 0.865\n",
            "Precision: 0.8665865384615384\n",
            "Recall: 0.8669172932330828\n",
            "Adjusted Mutual Information: 0.43569308728968414\n",
            "\n",
            "Experiment 2:\n",
            "Accuracy: 0.85\n",
            "Precision: 0.8521303258145363\n",
            "Recall: 0.8521303258145363\n",
            "Adjusted Mutual Information: 0.39733369487500664\n",
            "\n",
            "Experiment 3:\n",
            "Accuracy: 0.715\n",
            "Precision: 0.77\n",
            "Recall: 0.7030075187969924\n",
            "Adjusted Mutual Information: 0.18710101398305556\n",
            "\n",
            "Experiment 4:\n",
            "Accuracy: 0.505\n",
            "Precision: 0.5070450885668277\n",
            "Recall: 0.5070175438596491\n",
            "Adjusted Mutual Information: -0.0035157727375913165\n",
            "\n",
            "Experiment 5:\n",
            "Accuracy: 0.705\n",
            "Precision: 0.7321428571428572\n",
            "Recall: 0.6954887218045113\n",
            "Adjusted Mutual Information: 0.1426289602099171\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, adjusted_mutual_info_score\n",
        "from importlib import reload\n",
        "\n",
        "import experiments.classifiers.exp_hw6_1\n",
        "reload(experiments.classifiers.exp_hw6_1)\n",
        "\n",
        "import experiments.classifiers.exp_hw6_2\n",
        "reload(experiments.classifiers.exp_hw6_2)\n",
        "\n",
        "import experiments.classifiers.exp_hw6_3\n",
        "reload(experiments.classifiers.exp_hw6_3)\n",
        "\n",
        "import experiments.classifiers.exp_hw6_4\n",
        "reload(experiments.classifiers.exp_hw6_4)\n",
        "\n",
        "import experiments.classifiers.exp_hw6_5\n",
        "reload(experiments.classifiers.exp_hw6_5)\n",
        "\n",
        "with open('experiments/classifiers/exp_hw6_1.desc.yaml', 'r') as stream:\n",
        "    exp1_config = yaml.safe_load(stream)\n",
        "\n",
        "with open('experiments/classifiers/exp_hw6_2.desc.yaml', 'r') as stream:\n",
        "    exp2_config = yaml.safe_load(stream)\n",
        "\n",
        "with open('experiments/classifiers/exp_hw6_3.desc.yaml', 'r') as stream:\n",
        "    exp3_config = yaml.safe_load(stream)\n",
        "\n",
        "with open('experiments/classifiers/exp_hw6_4.desc.yaml', 'r') as stream:\n",
        "    exp4_config = yaml.safe_load(stream)\n",
        "\n",
        "with open('experiments/classifiers/exp_hw6_5.desc.yaml', 'r') as stream:\n",
        "    exp5_config = yaml.safe_load(stream)\n",
        "\n",
        "# I am using only the first 1000 rows for the experiments, but \n",
        "# you can use the full dataset if you want located in the data folder\n",
        "# with the name train_dataset.csv and test_dataset.csv.\n",
        "train_df = pd.read_csv('data/reduced_train_dataset.csv')\n",
        "test_df = pd.read_csv('data/reduced_test_dataset.csv')\n",
        "\n",
        "exp_hw6_1 = experiments.classifiers.exp_hw6_1.ExpHW6Classifier1(n_components=exp1_config['pca_n_components'])\n",
        "exp_hw6_2 = experiments.classifiers.exp_hw6_2.ExpHW6Classifier2(n_components=exp2_config['pca_n_components'])\n",
        "exp_hw6_3 = experiments.classifiers.exp_hw6_3.ExpHW6Classifier3(n_components=exp3_config['pca_n_components'])\n",
        "exp_hw6_4 = experiments.classifiers.exp_hw6_4.ExpHW6Classifier4(n_components=exp4_config['pca_n_components'])\n",
        "exp_hw6_5 = experiments.classifiers.exp_hw6_5.ExpHW6Classifier5(n_components=exp5_config['pca_n_components'])\n",
        "\n",
        "exp_hw6_1.fit(train_df, train_df['Target'])\n",
        "exp_hw6_2.fit(train_df, train_df['Target'])\n",
        "exp_hw6_3.fit(train_df, train_df['Target'])\n",
        "exp_hw6_4.fit(train_df, train_df['Target'])\n",
        "exp_hw6_5.fit(train_df, train_df['Target'])\n",
        "\n",
        "predictions_1 = exp_hw6_1.predict(test_df)\n",
        "predictions_2 = exp_hw6_2.predict(test_df)\n",
        "predictions_3 = exp_hw6_3.predict(test_df)\n",
        "predictions_4 = exp_hw6_4.predict(test_df)\n",
        "predictions_5 = exp_hw6_5.predict(test_df)\n",
        "\n",
        "import yaml\n",
        "\n",
        "for i, predictions in enumerate([predictions_1, predictions_2, predictions_3, predictions_4, predictions_5], start=1):\n",
        "    accuracy = accuracy_score(test_df['Target'], predictions)\n",
        "    precision = precision_score(test_df['Target'], predictions, average='macro')\n",
        "    recall = recall_score(test_df['Target'], predictions, average='macro')\n",
        "    ami = adjusted_mutual_info_score(test_df['Target'], predictions)\n",
        "\n",
        "    print(f\"Experiment {i}:\")\n",
        "    print(f\"Accuracy: {accuracy}\")\n",
        "    print(f\"Precision: {precision}\")\n",
        "    print(f\"Recall: {recall}\")\n",
        "    print(f\"Adjusted Mutual Information: {ami}\\n\")\n",
        "\n",
        "    yaml_content = {\n",
        "        'name': 'Results for experiment ' + str(i),\n",
        "        'description': 'Results for experiment ' + str(i),\n",
        "        'accuracy': accuracy.item(),\n",
        "        'precision': precision.item(),\n",
        "        'recall': recall.item(),\n",
        "        'ami': ami.item()\n",
        "    }\n",
        "\n",
        "    with open(f'experiments/classifiers/exp_hw6_{i}.results.yaml', 'w') as file:\n",
        "        yaml.dump(yaml_content, file, sort_keys=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sItXwf71KVFV"
      },
      "source": [
        "#### HW 10*: Read urls on dataset follow them with the following example, download attached images and count people on them with cv features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oR-z4PGeKVFV"
      },
      "source": [
        "#### HW 11: Add new features to all PCA output on HW 6, train classifiers, test quality and store results in proper .yaml files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {},
      "outputs": [],
      "source": [
        "!mkdir -p experiments/classifiers\n",
        "\n",
        "import yaml\n",
        "\n",
        "yaml_content = {\n",
        "    'name': 'Experiment 6',\n",
        "    'description': 'This is the sixth experiment',\n",
        "    'pca_n_components': 15,\n",
        "    'extra_features': ['isRetweet', 'retweeted']\n",
        "}\n",
        "\n",
        "with open('experiments/classifiers/exp_hw6_6_extra_features.desc.yaml', 'w') as file:\n",
        "    yaml.dump(yaml_content, file, sort_keys=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {},
      "outputs": [],
      "source": [
        "!mkdir -p experiments/classifiers\n",
        "\n",
        "import yaml\n",
        "\n",
        "yaml_content = {\n",
        "    'name': 'Experiment 7',\n",
        "    'description': 'This is the seventh experiment',\n",
        "    'pca_n_components': 15,\n",
        "    'extra_features': ['isRetweet', 'retweeted']\n",
        "}\n",
        "\n",
        "with open('experiments/classifiers/exp_hw6_7_extra_features.desc.yaml', 'w') as file:\n",
        "    yaml.dump(yaml_content, file, sort_keys=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {},
      "outputs": [],
      "source": [
        "!mkdir -p experiments/classifiers\n",
        "\n",
        "import yaml\n",
        "\n",
        "yaml_content = {\n",
        "    'name': 'Experiment 8',\n",
        "    'description': 'This is the eighth experiment',\n",
        "    'pca_n_components': 15,\n",
        "    'extra_features': ['isRetweet', 'retweeted']\n",
        "}\n",
        "\n",
        "with open('experiments/classifiers/exp_hw6_8_extra_features.desc.yaml', 'w') as file:\n",
        "    yaml.dump(yaml_content, file, sort_keys=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {},
      "outputs": [],
      "source": [
        "!mkdir -p experiments/classifiers\n",
        "\n",
        "import yaml\n",
        "\n",
        "yaml_content = {\n",
        "    'name': 'Experiment 9',\n",
        "    'description': 'This is the ninth experiment',\n",
        "    'pca_n_components': 15,\n",
        "    'extra_features': ['isRetweet', 'retweeted']\n",
        "}\n",
        "\n",
        "with open('experiments/classifiers/exp_hw6_9_extra_features.desc.yaml', 'w') as file:\n",
        "    yaml.dump(yaml_content, file, sort_keys=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {},
      "outputs": [],
      "source": [
        "!mkdir -p experiments/classifiers\n",
        "\n",
        "import yaml\n",
        "\n",
        "yaml_content = {\n",
        "    'name': 'Experiment 10',\n",
        "    'description': 'This is the tenth experiment',\n",
        "    'pca_n_components': 15,\n",
        "    'extra_features': ['isRetweet', 'retweeted']\n",
        "}\n",
        "\n",
        "with open('experiments/classifiers/exp_hw6_10_extra_features.desc.yaml', 'w') as file:\n",
        "    yaml.dump(yaml_content, file, sort_keys=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 220,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/brayand/.local/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment 6:\n",
            "Accuracy: 0.88\n",
            "Precision: 0.8797879787978797\n",
            "Recall: 0.880701754385965\n",
            "Adjusted Mutual Information: 0.4704333250098475\n",
            "\n",
            "Experiment 7:\n",
            "Accuracy: 0.855\n",
            "Precision: 0.8556422569027611\n",
            "Recall: 0.856390977443609\n",
            "Adjusted Mutual Information: 0.40532985639750835\n",
            "\n",
            "Experiment 8:\n",
            "Accuracy: 0.84\n",
            "Precision: 0.8560000000000001\n",
            "Recall: 0.8345864661654135\n",
            "Adjusted Mutual Information: 0.39346571495462085\n",
            "\n",
            "Experiment 9:\n",
            "Accuracy: 0.55\n",
            "Precision: 0.5541653951785169\n",
            "Recall: 0.5533834586466165\n",
            "Adjusted Mutual Information: 0.004784818728299431\n",
            "\n",
            "Experiment 10:\n",
            "Accuracy: 0.705\n",
            "Precision: 0.7321428571428572\n",
            "Recall: 0.6954887218045113\n",
            "Adjusted Mutual Information: 0.1426289602099171\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, adjusted_mutual_info_score\n",
        "from importlib import reload\n",
        "\n",
        "import experiments.classifiers.exp_hw6_1\n",
        "reload(experiments.classifiers.exp_hw6_1)\n",
        "\n",
        "import experiments.classifiers.exp_hw6_2\n",
        "reload(experiments.classifiers.exp_hw6_2)\n",
        "\n",
        "import experiments.classifiers.exp_hw6_3\n",
        "reload(experiments.classifiers.exp_hw6_3)\n",
        "\n",
        "import experiments.classifiers.exp_hw6_4\n",
        "reload(experiments.classifiers.exp_hw6_4)\n",
        "\n",
        "import experiments.classifiers.exp_hw6_5\n",
        "reload(experiments.classifiers.exp_hw6_5)\n",
        "\n",
        "with open('experiments/classifiers/exp_hw6_6_extra_features.desc.yaml', 'r') as stream:\n",
        "    exp6_config = yaml.safe_load(stream)\n",
        "\n",
        "with open('experiments/classifiers/exp_hw6_7_extra_features.desc.yaml', 'r') as stream:\n",
        "    exp7_config = yaml.safe_load(stream)\n",
        "\n",
        "with open('experiments/classifiers/exp_hw6_8_extra_features.desc.yaml', 'r') as stream:\n",
        "    exp8_config = yaml.safe_load(stream)\n",
        "\n",
        "with open('experiments/classifiers/exp_hw6_9_extra_features.desc.yaml', 'r') as stream:\n",
        "    exp9_config = yaml.safe_load(stream)\n",
        "\n",
        "with open('experiments/classifiers/exp_hw6_10_extra_features.desc.yaml', 'r') as stream:\n",
        "    exp10_config = yaml.safe_load(stream)\n",
        "\n",
        "# I am using only the first 1000 rows for the experiments, but \n",
        "# you can use the full dataset if you want located in the data folder\n",
        "# with the name train_dataset.csv and test_dataset.csv.\n",
        "train_df = pd.read_csv('data/reduced_train_dataset.csv')\n",
        "test_df = pd.read_csv('data/reduced_test_dataset.csv')\n",
        "\n",
        "exp_hw6_6 = experiments.classifiers.exp_hw6_1.ExpHW6Classifier1(n_components=exp6_config['pca_n_components'], extra_features=train_df[exp6_config['extra_features']])\n",
        "exp_hw6_7 = experiments.classifiers.exp_hw6_2.ExpHW6Classifier2(n_components=exp7_config['pca_n_components'], extra_features=train_df[exp7_config['extra_features']])\n",
        "exp_hw6_8 = experiments.classifiers.exp_hw6_3.ExpHW6Classifier3(n_components=exp8_config['pca_n_components'], extra_features=train_df[exp8_config['extra_features']])\n",
        "exp_hw6_9 = experiments.classifiers.exp_hw6_4.ExpHW6Classifier4(n_components=exp9_config['pca_n_components'], extra_features=train_df[exp9_config['extra_features']])\n",
        "exp_hw6_10 = experiments.classifiers.exp_hw6_5.ExpHW6Classifier5(n_components=exp10_config['pca_n_components'], extra_features=train_df[exp10_config['extra_features']])\n",
        "\n",
        "exp_hw6_6.fit(train_df, train_df['Target'])\n",
        "exp_hw6_7.fit(train_df, train_df['Target'])\n",
        "exp_hw6_8.fit(train_df, train_df['Target'])\n",
        "exp_hw6_9.fit(train_df, train_df['Target'])\n",
        "exp_hw6_10.fit(train_df, train_df['Target'])\n",
        "\n",
        "predictions_6 = exp_hw6_6.predict(test_df)\n",
        "predictions_7 = exp_hw6_7.predict(test_df)\n",
        "predictions_8 = exp_hw6_8.predict(test_df)\n",
        "predictions_9 = exp_hw6_9.predict(test_df)\n",
        "predictions_10 = exp_hw6_10.predict(test_df)\n",
        "\n",
        "import yaml\n",
        "\n",
        "for i, predictions in enumerate([predictions_6, predictions_7, predictions_8, predictions_9, predictions_10], start=1):\n",
        "    accuracy = accuracy_score(test_df['Target'], predictions)\n",
        "    precision = precision_score(test_df['Target'], predictions, average='macro')\n",
        "    recall = recall_score(test_df['Target'], predictions, average='macro')\n",
        "    ami = adjusted_mutual_info_score(test_df['Target'], predictions)\n",
        "\n",
        "    print(f\"Experiment {5 + i}:\")\n",
        "    print(f\"Accuracy: {accuracy}\")\n",
        "    print(f\"Precision: {precision}\")\n",
        "    print(f\"Recall: {recall}\")\n",
        "    print(f\"Adjusted Mutual Information: {ami}\\n\")\n",
        "\n",
        "    yaml_content = {\n",
        "        'name': 'Results for experiment ' + str(i),\n",
        "        'description': 'Results for experiment ' + str(i),\n",
        "        'accuracy': accuracy.item(),\n",
        "        'precision': precision.item(),\n",
        "        'recall': recall.item(),\n",
        "        'ami': ami.item()\n",
        "    }\n",
        "\n",
        "    with open(f'experiments/classifiers/exp_hw6_{5 + i}_extra_features.results.yaml', 'w') as file:\n",
        "        yaml.dump(yaml_content, file, sort_keys=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bx56ahDiKVFW"
      },
      "source": [
        "#### HW 12*: Perform topic modelings on text feature, add it to PCA features from HW 6, train classifiers, test quality and store results in proper .yaml files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 233,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "import pandas as pd\n",
        "\n",
        "dataset_with_topics_df = pd.read_csv('data/tweets_with_characters_and_target.csv')\n",
        "\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "X = vectorizer.fit_transform(dataset_with_topics_df['text'])\n",
        "\n",
        "lda = LatentDirichletAllocation(n_components=10, random_state=0)\n",
        "lda.fit(X)\n",
        "\n",
        "topic_distribution = lda.transform(X)\n",
        "\n",
        "for text_idx, distribution in enumerate(topic_distribution):\n",
        "    for topic_idx, value in enumerate(distribution):\n",
        "        dataset_with_topics_df.loc[text_idx, f\"topic_{topic_idx + 1}\"] = value\n",
        "\n",
        "!mkdir -p data\n",
        "\n",
        "dataset_with_topics_df.to_csv('data/dataset_with_topics.csv', index=False)\n",
        "\n",
        "import yaml\n",
        "\n",
        "yaml_content = {\n",
        "    'name': 'Dataset with topics',\n",
        "    'description': 'Dataset with topics',\n",
        "    'num_rows': len(dataset_with_topics_df),\n",
        "    'num_columns': len(dataset_with_topics_df.columns),\n",
        "    'columns': list(dataset_with_topics_df.columns)\n",
        "}\n",
        "\n",
        "with open('data/dataset_with_topics.desc.yaml', 'w') as file:\n",
        "    yaml.dump(yaml_content, file, sort_keys=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 236,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset_with_topics_df = pd.read_csv('data/dataset_with_topics.csv')\n",
        "\n",
        "reduced_dataset_with_topics_df = dataset_with_topics_df.copy().iloc[0:1000]\n",
        "\n",
        "reduced_dataset_with_topics_df.to_csv('data/reduced_dataset_with_topics.csv', index=False)\n",
        "\n",
        "import yaml\n",
        "\n",
        "yaml_content = {\n",
        "    'name': 'Reduced_dataset_with_topics',\n",
        "    'description': 'Reduced_dataset_with_topics',\n",
        "    'num_rows': len(reduced_dataset_with_topics_df),\n",
        "    'num_columns': len(reduced_dataset_with_topics_df.columns),\n",
        "    'columns': list(reduced_dataset_with_topics_df.columns)\n",
        "}\n",
        "\n",
        "with open('data/reduced_dataset_with_topics.desc.yaml', 'w') as file:\n",
        "    yaml.dump(yaml_content, file, sort_keys=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 237,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>text</th>\n",
              "      <th>favorited</th>\n",
              "      <th>favoriteCount</th>\n",
              "      <th>replyToSN</th>\n",
              "      <th>created</th>\n",
              "      <th>truncated</th>\n",
              "      <th>replyToSID</th>\n",
              "      <th>id</th>\n",
              "      <th>replyToUID</th>\n",
              "      <th>...</th>\n",
              "      <th>topic_1</th>\n",
              "      <th>topic_2</th>\n",
              "      <th>topic_3</th>\n",
              "      <th>topic_4</th>\n",
              "      <th>topic_5</th>\n",
              "      <th>topic_6</th>\n",
              "      <th>topic_7</th>\n",
              "      <th>topic_8</th>\n",
              "      <th>topic_9</th>\n",
              "      <th>topic_10</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>RT @mrvelstan: literally nobody:\\r\\nme:\\r\\n\\r\\...</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2019-04-23 10:43:30</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1120639328034676737</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.014289</td>\n",
              "      <td>0.014288</td>\n",
              "      <td>0.014287</td>\n",
              "      <td>0.014288</td>\n",
              "      <td>0.871407</td>\n",
              "      <td>0.014288</td>\n",
              "      <td>0.014289</td>\n",
              "      <td>0.014288</td>\n",
              "      <td>0.014287</td>\n",
              "      <td>0.014289</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>RT @agntecarter: im emotional, sorry!!\\r\\n\\r\\...</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2019-04-23 10:43:30</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1120639325199196160</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.009094</td>\n",
              "      <td>0.009092</td>\n",
              "      <td>0.009091</td>\n",
              "      <td>0.009092</td>\n",
              "      <td>0.918173</td>\n",
              "      <td>0.009092</td>\n",
              "      <td>0.009092</td>\n",
              "      <td>0.009091</td>\n",
              "      <td>0.009091</td>\n",
              "      <td>0.009093</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>saving these bingo cards for tomorrow \\r\\n©\\r\\...</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2019-04-23 10:43:30</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1120639324683292674</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.012501</td>\n",
              "      <td>0.012501</td>\n",
              "      <td>0.012501</td>\n",
              "      <td>0.012501</td>\n",
              "      <td>0.887492</td>\n",
              "      <td>0.012501</td>\n",
              "      <td>0.012501</td>\n",
              "      <td>0.012501</td>\n",
              "      <td>0.012500</td>\n",
              "      <td>0.012501</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>RT @HelloBoon: Man these #AvengersEndgame ads ...</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2019-04-23 10:43:29</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1120639323328540672</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.012502</td>\n",
              "      <td>0.012501</td>\n",
              "      <td>0.012501</td>\n",
              "      <td>0.012501</td>\n",
              "      <td>0.012501</td>\n",
              "      <td>0.012501</td>\n",
              "      <td>0.887489</td>\n",
              "      <td>0.012501</td>\n",
              "      <td>0.012501</td>\n",
              "      <td>0.012501</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>RT @Marvel: We salute you, @ChrisEvans! #Capta...</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2019-04-23 10:43:29</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1120639321571074048</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.899991</td>\n",
              "      <td>0.011112</td>\n",
              "      <td>0.011112</td>\n",
              "      <td>0.011112</td>\n",
              "      <td>0.011112</td>\n",
              "      <td>0.011112</td>\n",
              "      <td>0.011112</td>\n",
              "      <td>0.011112</td>\n",
              "      <td>0.011112</td>\n",
              "      <td>0.011112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>996</td>\n",
              "      <td>Anyone interested for #AvengersEndgame 2D tick...</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2019-04-23 10:38:29</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1120638063556710405</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.011117</td>\n",
              "      <td>0.011111</td>\n",
              "      <td>0.011113</td>\n",
              "      <td>0.011112</td>\n",
              "      <td>0.649040</td>\n",
              "      <td>0.262057</td>\n",
              "      <td>0.011112</td>\n",
              "      <td>0.011114</td>\n",
              "      <td>0.011111</td>\n",
              "      <td>0.011112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>997</td>\n",
              "      <td>RT @Avengers: .@Renner4Real hits the mark! #Ha...</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2019-04-23 10:38:28</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1120638060503425029</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.010002</td>\n",
              "      <td>0.909987</td>\n",
              "      <td>0.010001</td>\n",
              "      <td>0.010002</td>\n",
              "      <td>0.010001</td>\n",
              "      <td>0.010001</td>\n",
              "      <td>0.010002</td>\n",
              "      <td>0.010001</td>\n",
              "      <td>0.010001</td>\n",
              "      <td>0.010001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>998</td>\n",
              "      <td>RT @MCU_Direct: The first NON-SPOILER #Avenger...</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2019-04-23 10:38:27</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1120638057315803137</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.008334</td>\n",
              "      <td>0.008334</td>\n",
              "      <td>0.008334</td>\n",
              "      <td>0.008334</td>\n",
              "      <td>0.008334</td>\n",
              "      <td>0.924995</td>\n",
              "      <td>0.008334</td>\n",
              "      <td>0.008334</td>\n",
              "      <td>0.008334</td>\n",
              "      <td>0.008334</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>999</td>\n",
              "      <td>RT @Marvel: These guys. @AnthonyMackie @chrish...</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2019-04-23 10:38:27</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1120638057051410434</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.011114</td>\n",
              "      <td>0.011113</td>\n",
              "      <td>0.011112</td>\n",
              "      <td>0.899986</td>\n",
              "      <td>0.011113</td>\n",
              "      <td>0.011113</td>\n",
              "      <td>0.011113</td>\n",
              "      <td>0.011112</td>\n",
              "      <td>0.011112</td>\n",
              "      <td>0.011113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>1000</td>\n",
              "      <td>RT @hmvtweets: WIN an #AvengersEndgame merch b...</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2019-04-23 10:38:27</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1120638056778862593</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.006667</td>\n",
              "      <td>0.006667</td>\n",
              "      <td>0.006667</td>\n",
              "      <td>0.006667</td>\n",
              "      <td>0.006667</td>\n",
              "      <td>0.006667</td>\n",
              "      <td>0.939996</td>\n",
              "      <td>0.006667</td>\n",
              "      <td>0.006668</td>\n",
              "      <td>0.006667</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 62 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Unnamed: 0                                               text  favorited  \\\n",
              "0             1  RT @mrvelstan: literally nobody:\\r\\nme:\\r\\n\\r\\...      False   \n",
              "1             2  RT @agntecarter: im emotional, sorry!!\\r\\n\\r\\...      False   \n",
              "2             3  saving these bingo cards for tomorrow \\r\\n©\\r\\...      False   \n",
              "3             4  RT @HelloBoon: Man these #AvengersEndgame ads ...      False   \n",
              "4             5  RT @Marvel: We salute you, @ChrisEvans! #Capta...      False   \n",
              "..          ...                                                ...        ...   \n",
              "995         996  Anyone interested for #AvengersEndgame 2D tick...      False   \n",
              "996         997  RT @Avengers: .@Renner4Real hits the mark! #Ha...      False   \n",
              "997         998  RT @MCU_Direct: The first NON-SPOILER #Avenger...      False   \n",
              "998         999  RT @Marvel: These guys. @AnthonyMackie @chrish...      False   \n",
              "999        1000  RT @hmvtweets: WIN an #AvengersEndgame merch b...      False   \n",
              "\n",
              "     favoriteCount replyToSN              created  truncated  replyToSID  \\\n",
              "0                0       NaN  2019-04-23 10:43:30      False         NaN   \n",
              "1                0       NaN  2019-04-23 10:43:30      False         NaN   \n",
              "2                0       NaN  2019-04-23 10:43:30      False         NaN   \n",
              "3                0       NaN  2019-04-23 10:43:29      False         NaN   \n",
              "4                0       NaN  2019-04-23 10:43:29      False         NaN   \n",
              "..             ...       ...                  ...        ...         ...   \n",
              "995              0       NaN  2019-04-23 10:38:29      False         NaN   \n",
              "996              0       NaN  2019-04-23 10:38:28      False         NaN   \n",
              "997              0       NaN  2019-04-23 10:38:27      False         NaN   \n",
              "998              0       NaN  2019-04-23 10:38:27      False         NaN   \n",
              "999              0       NaN  2019-04-23 10:38:27      False         NaN   \n",
              "\n",
              "                      id  replyToUID  ...   topic_1   topic_2   topic_3  \\\n",
              "0    1120639328034676737         NaN  ...  0.014289  0.014288  0.014287   \n",
              "1    1120639325199196160         NaN  ...  0.009094  0.009092  0.009091   \n",
              "2    1120639324683292674         NaN  ...  0.012501  0.012501  0.012501   \n",
              "3    1120639323328540672         NaN  ...  0.012502  0.012501  0.012501   \n",
              "4    1120639321571074048         NaN  ...  0.899991  0.011112  0.011112   \n",
              "..                   ...         ...  ...       ...       ...       ...   \n",
              "995  1120638063556710405         NaN  ...  0.011117  0.011111  0.011113   \n",
              "996  1120638060503425029         NaN  ...  0.010002  0.909987  0.010001   \n",
              "997  1120638057315803137         NaN  ...  0.008334  0.008334  0.008334   \n",
              "998  1120638057051410434         NaN  ...  0.011114  0.011113  0.011112   \n",
              "999  1120638056778862593         NaN  ...  0.006667  0.006667  0.006667   \n",
              "\n",
              "      topic_4   topic_5   topic_6   topic_7   topic_8   topic_9  topic_10  \n",
              "0    0.014288  0.871407  0.014288  0.014289  0.014288  0.014287  0.014289  \n",
              "1    0.009092  0.918173  0.009092  0.009092  0.009091  0.009091  0.009093  \n",
              "2    0.012501  0.887492  0.012501  0.012501  0.012501  0.012500  0.012501  \n",
              "3    0.012501  0.012501  0.012501  0.887489  0.012501  0.012501  0.012501  \n",
              "4    0.011112  0.011112  0.011112  0.011112  0.011112  0.011112  0.011112  \n",
              "..        ...       ...       ...       ...       ...       ...       ...  \n",
              "995  0.011112  0.649040  0.262057  0.011112  0.011114  0.011111  0.011112  \n",
              "996  0.010002  0.010001  0.010001  0.010002  0.010001  0.010001  0.010001  \n",
              "997  0.008334  0.008334  0.924995  0.008334  0.008334  0.008334  0.008334  \n",
              "998  0.899986  0.011113  0.011113  0.011113  0.011112  0.011112  0.011113  \n",
              "999  0.006667  0.006667  0.006667  0.939996  0.006667  0.006668  0.006667  \n",
              "\n",
              "[1000 rows x 62 columns]"
            ]
          },
          "execution_count": 237,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "topic_train_df, topic_test_df = train_test_split(reduced_dataset_with_topics_df, test_size=0.2, random_state=42)\n",
        "\n",
        "reduced_dataset_with_topics_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 238,
      "metadata": {},
      "outputs": [],
      "source": [
        "!mkdir -p experiments/classifiers\n",
        "\n",
        "import yaml\n",
        "\n",
        "yaml_content = {\n",
        "    'name': 'Experiment 11',\n",
        "    'description': 'This another experiment',\n",
        "    'pca_n_components': 15,\n",
        "    'extra_features': ['isRetweet', 'retweeted', 'topic_1', 'topic_2', 'topic_3', 'topic_4', 'topic_5', 'topic_6', 'topic_7', 'topic_8', 'topic_9', 'topic_10']\n",
        "}\n",
        "\n",
        "with open('experiments/classifiers/exp_hw6_11_extra_features.desc.yaml', 'w') as file:\n",
        "    yaml.dump(yaml_content, file, sort_keys=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 239,
      "metadata": {},
      "outputs": [],
      "source": [
        "!mkdir -p experiments/classifiers\n",
        "\n",
        "import yaml\n",
        "\n",
        "yaml_content = {\n",
        "    'name': 'Experiment 12',\n",
        "    'description': 'This another experiment',\n",
        "    'pca_n_components': 15,\n",
        "    'extra_features': ['isRetweet', 'retweeted', 'topic_1', 'topic_2', 'topic_3', 'topic_4', 'topic_5', 'topic_6', 'topic_7', 'topic_8', 'topic_9', 'topic_10']\n",
        "}\n",
        "\n",
        "with open('experiments/classifiers/exp_hw6_12_extra_features.desc.yaml', 'w') as file:\n",
        "    yaml.dump(yaml_content, file, sort_keys=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 240,
      "metadata": {},
      "outputs": [],
      "source": [
        "!mkdir -p experiments/classifiers\n",
        "\n",
        "import yaml\n",
        "\n",
        "yaml_content = {\n",
        "    'name': 'Experiment 13',\n",
        "    'description': 'This another experiment',\n",
        "    'pca_n_components': 15,\n",
        "    'extra_features': ['isRetweet', 'retweeted', 'topic_1', 'topic_2', 'topic_3', 'topic_4', 'topic_5', 'topic_6', 'topic_7', 'topic_8', 'topic_9', 'topic_10']\n",
        "}\n",
        "\n",
        "with open('experiments/classifiers/exp_hw6_13_extra_features.desc.yaml', 'w') as file:\n",
        "    yaml.dump(yaml_content, file, sort_keys=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 241,
      "metadata": {},
      "outputs": [],
      "source": [
        "!mkdir -p experiments/classifiers\n",
        "\n",
        "import yaml\n",
        "\n",
        "yaml_content = {\n",
        "    'name': 'Experiment 14',\n",
        "    'description': 'This another experiment',\n",
        "    'pca_n_components': 15,\n",
        "    'extra_features': ['isRetweet', 'retweeted', 'topic_1', 'topic_2', 'topic_3', 'topic_4', 'topic_5', 'topic_6', 'topic_7', 'topic_8', 'topic_9', 'topic_10']\n",
        "}\n",
        "\n",
        "with open('experiments/classifiers/exp_hw6_14_extra_features.desc.yaml', 'w') as file:\n",
        "    yaml.dump(yaml_content, file, sort_keys=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 242,
      "metadata": {},
      "outputs": [],
      "source": [
        "!mkdir -p experiments/classifiers\n",
        "\n",
        "import yaml\n",
        "\n",
        "yaml_content = {\n",
        "    'name': 'Experiment 15',\n",
        "    'description': 'This another experiment',\n",
        "    'pca_n_components': 15,\n",
        "    'extra_features': ['isRetweet', 'retweeted', 'topic_1', 'topic_2', 'topic_3', 'topic_4', 'topic_5', 'topic_6', 'topic_7', 'topic_8', 'topic_9', 'topic_10']\n",
        "}\n",
        "\n",
        "with open('experiments/classifiers/exp_hw6_15_extra_features.desc.yaml', 'w') as file:\n",
        "    yaml.dump(yaml_content, file, sort_keys=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 243,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/brayand/.local/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment 11:\n",
            "Accuracy: 0.88\n",
            "Precision: 0.8797879787978797\n",
            "Recall: 0.880701754385965\n",
            "Adjusted Mutual Information: 0.4704333250098475\n",
            "\n",
            "Experiment 12:\n",
            "Accuracy: 0.855\n",
            "Precision: 0.8556422569027611\n",
            "Recall: 0.856390977443609\n",
            "Adjusted Mutual Information: 0.40532985639750835\n",
            "\n",
            "Experiment 13:\n",
            "Accuracy: 0.84\n",
            "Precision: 0.8560000000000001\n",
            "Recall: 0.8345864661654135\n",
            "Adjusted Mutual Information: 0.39346571495462085\n",
            "\n",
            "Experiment 14:\n",
            "Accuracy: 0.53\n",
            "Precision: 0.5345268542199488\n",
            "Recall: 0.5338345864661654\n",
            "Adjusted Mutual Information: -0.00026234789916560294\n",
            "\n",
            "Experiment 15:\n",
            "Accuracy: 0.705\n",
            "Precision: 0.7321428571428572\n",
            "Recall: 0.6954887218045113\n",
            "Adjusted Mutual Information: 0.1426289602099171\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, adjusted_mutual_info_score\n",
        "from importlib import reload\n",
        "\n",
        "import experiments.classifiers.exp_hw6_1\n",
        "reload(experiments.classifiers.exp_hw6_1)\n",
        "\n",
        "import experiments.classifiers.exp_hw6_2\n",
        "reload(experiments.classifiers.exp_hw6_2)\n",
        "\n",
        "import experiments.classifiers.exp_hw6_3\n",
        "reload(experiments.classifiers.exp_hw6_3)\n",
        "\n",
        "import experiments.classifiers.exp_hw6_4\n",
        "reload(experiments.classifiers.exp_hw6_4)\n",
        "\n",
        "import experiments.classifiers.exp_hw6_5\n",
        "reload(experiments.classifiers.exp_hw6_5)\n",
        "\n",
        "with open('experiments/classifiers/exp_hw6_11_extra_features.desc.yaml', 'r') as stream:\n",
        "    exp11_config = yaml.safe_load(stream)\n",
        "\n",
        "with open('experiments/classifiers/exp_hw6_12_extra_features.desc.yaml', 'r') as stream:\n",
        "    exp12_config = yaml.safe_load(stream)\n",
        "\n",
        "with open('experiments/classifiers/exp_hw6_13_extra_features.desc.yaml', 'r') as stream:\n",
        "    exp13_config = yaml.safe_load(stream)\n",
        "\n",
        "with open('experiments/classifiers/exp_hw6_14_extra_features.desc.yaml', 'r') as stream:\n",
        "    exp14_config = yaml.safe_load(stream)\n",
        "\n",
        "with open('experiments/classifiers/exp_hw6_15_extra_features.desc.yaml', 'r') as stream:\n",
        "    exp15_config = yaml.safe_load(stream)\n",
        "\n",
        "# I am using only the first 1000 rows for the experiments, but \n",
        "# you can use the full dataset if you want located in the data folder\n",
        "# with the name train_dataset.csv and test_dataset.csv.\n",
        "train_df = topic_train_df\n",
        "test_df = topic_test_df\n",
        "\n",
        "exp_hw6_11 = experiments.classifiers.exp_hw6_1.ExpHW6Classifier1(n_components=exp11_config['pca_n_components'], extra_features=train_df[exp11_config['extra_features']])\n",
        "exp_hw6_12 = experiments.classifiers.exp_hw6_2.ExpHW6Classifier2(n_components=exp12_config['pca_n_components'], extra_features=train_df[exp12_config['extra_features']])\n",
        "exp_hw6_13 = experiments.classifiers.exp_hw6_3.ExpHW6Classifier3(n_components=exp13_config['pca_n_components'], extra_features=train_df[exp13_config['extra_features']])\n",
        "exp_hw6_14 = experiments.classifiers.exp_hw6_4.ExpHW6Classifier4(n_components=exp14_config['pca_n_components'], extra_features=train_df[exp14_config['extra_features']])\n",
        "exp_hw6_15 = experiments.classifiers.exp_hw6_5.ExpHW6Classifier5(n_components=exp15_config['pca_n_components'], extra_features=train_df[exp15_config['extra_features']])\n",
        "\n",
        "exp_hw6_11.fit(train_df, train_df['Target'])\n",
        "exp_hw6_12.fit(train_df, train_df['Target'])\n",
        "exp_hw6_13.fit(train_df, train_df['Target'])\n",
        "exp_hw6_14.fit(train_df, train_df['Target'])\n",
        "exp_hw6_15.fit(train_df, train_df['Target'])\n",
        "\n",
        "predictions_11 = exp_hw6_11.predict(test_df)\n",
        "predictions_12 = exp_hw6_12.predict(test_df)\n",
        "predictions_13 = exp_hw6_13.predict(test_df)\n",
        "predictions_14 = exp_hw6_14.predict(test_df)\n",
        "predictions_15 = exp_hw6_15.predict(test_df)\n",
        "\n",
        "import yaml\n",
        "\n",
        "for i, predictions in enumerate([predictions_11, predictions_12, predictions_13, predictions_14, predictions_15], start=1):\n",
        "    accuracy = accuracy_score(test_df['Target'], predictions)\n",
        "    precision = precision_score(test_df['Target'], predictions, average='macro')\n",
        "    recall = recall_score(test_df['Target'], predictions, average='macro')\n",
        "    ami = adjusted_mutual_info_score(test_df['Target'], predictions)\n",
        "\n",
        "    print(f\"Experiment {10 + i}:\")\n",
        "    print(f\"Accuracy: {accuracy}\")\n",
        "    print(f\"Precision: {precision}\")\n",
        "    print(f\"Recall: {recall}\")\n",
        "    print(f\"Adjusted Mutual Information: {ami}\\n\")\n",
        "\n",
        "    yaml_content = {\n",
        "        'name': 'Results for experiment ' + str(i),\n",
        "        'description': 'Results for experiment ' + str(i),\n",
        "        'accuracy': accuracy.item(),\n",
        "        'precision': precision.item(),\n",
        "        'recall': recall.item(),\n",
        "        'ami': ami.item()\n",
        "    }\n",
        "\n",
        "    with open(f'experiments/classifiers/exp_hw6_{10 + i}_topic_modeling.results.yaml', 'w') as file:\n",
        "        yaml.dump(yaml_content, file, sort_keys=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t23JCuXmKVFW"
      },
      "source": [
        "#### HW 13: Combine all combinations of features from HW6, measure the results of improvement of classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 246,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(800, 10)\n",
            "(800, 10)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/brayand/.local/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(800, 10)\n",
            "(800, 10)\n",
            "(800, 10)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(800, 50)"
            ]
          },
          "execution_count": 246,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from importlib import reload\n",
        "\n",
        "import experiments.transformers.word_vectorizer_1gram\n",
        "reload(experiments.transformers.word_vectorizer_1gram)\n",
        "\n",
        "import experiments.transformers.np_vectorizer\n",
        "reload(experiments.transformers.np_vectorizer)\n",
        "\n",
        "import experiments.transformers.word_vectorizer_2gram\n",
        "reload(experiments.transformers.word_vectorizer_2gram)\n",
        "\n",
        "import experiments.transformers.word_vectorizer_2gram_by_character\n",
        "reload(experiments.transformers.word_vectorizer_2gram_by_character)\n",
        "\n",
        "import experiments.transformers.polynomial_features_by_character\n",
        "reload(experiments.transformers.polynomial_features_by_character)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "transformer = {}\n",
        "\n",
        "transformer[0] = Pipeline([\n",
        "    ('word_vectorizer_1gram', experiments.transformers.word_vectorizer_1gram.WordVectorizer1Gram()),\n",
        "    ('truncated_svd', TruncatedSVD(n_components=10)),\n",
        "])\n",
        "\n",
        "transformer[1] = Pipeline([\n",
        "    ('np_vectorizer', experiments.transformers.np_vectorizer.NPVectorizer()),\n",
        "    ('truncated_svd', TruncatedSVD(n_components=10)),\n",
        "])\n",
        "transformer[2] = Pipeline([\n",
        "    ('word_vectorizer_2gram', experiments.transformers.word_vectorizer_2gram.WordVectorizer2Gram()),\n",
        "    ('truncated_svd', TruncatedSVD(n_components=10)),\n",
        "])\n",
        "\n",
        "transformer[3] = Pipeline([\n",
        "    ('word_vectorizer_2gram_by_character', experiments.transformers.word_vectorizer_2gram_by_character.WordVectorizer2GramByCharacter()),\n",
        "    ('truncated_svd', TruncatedSVD(n_components=10)),\n",
        "])\n",
        "\n",
        "transformer[4] = Pipeline([\n",
        "    ('polynomial_features_by_character', experiments.transformers.polynomial_features_by_character.PolynomialFeaturesByCharacter()),\n",
        "    ('truncated_svd', TruncatedSVD(n_components=10)),\n",
        "])\n",
        "\n",
        "concated_results = None\n",
        "\n",
        "for i in range(5):\n",
        "\n",
        "    result = transformer[i].fit_transform(train_df)\n",
        "\n",
        "    if concated_results is None:\n",
        "        concated_results = result\n",
        "    else:\n",
        "        concated_results = np.hstack((concated_results, result))\n",
        "\n",
        "\n",
        "classifier = LogisticRegression()\n",
        "\n",
        "classifier.fit(concated_results, train_df['Target'])\n",
        "\n",
        "predictions = classifier.predict(concated_results)\n",
        "\n",
        "accuracy = accuracy_score(train_df['Target'], predictions)\n",
        "precision = precision_score(train_df['Target'], predictions, average='macro')\n",
        "recall = recall_score(train_df['Target'], predictions, average='macro')\n",
        "ami = adjusted_mutual_info_score(train_df['Target'], predictions)\n",
        "\n",
        "print(f\"Experiment 16:\")\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"Adjusted Mutual Information: {ami}\\n\")\n",
        "\n",
        "yaml_content = {\n",
        "    'name': 'Results for experiment 16',\n",
        "    'description': 'Results for experiment 16',\n",
        "    'accuracy': accuracy.item(),\n",
        "    'precision': precision.item(),\n",
        "    'recall': recall.item(),\n",
        "    'ami': ami.item()\n",
        "}\n",
        "\n",
        "with open(f'experiments/classifiers/exp_hw6_16_combined.results.yaml', 'w') as file:\n",
        "    yaml.dump(yaml_content, file, sort_keys=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gf6uIqDBKVFW"
      },
      "source": [
        "#### HW 14*: For 5 best of prevous experiments, change LinearRegression to XGBClassifier. Try several XGB configurations, store results and parameters in .yaml files"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
